[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "",
    "text": "Preface\nThe Unjournal coordinates the public evaluation of hosted papers and dynamically-presented research projects. We are working independently of traditional academic journals to build an open platform and a sustainable system for feedback, ratings, and assessment. Our initial focus is quantitative work that informs global priorities, especially in economics, policy, and other social sciences. We will encourage better research by making it easier for researchers to get feedback and credible ratings on their work. Our aim: to make rigorous research more impactful, and impactful research more rigorous.\nOur main web site, unjournal.org as well as our knowledge base, explain and present our vision, procedures, and our progress. The ‘output’ evaluations and author (including feedback and discussion) can be found on our PubPub page, and are indexed in scholarly archives.\nAlso see our interactive dashboards\nThis site and the accompanying dashboards present data and analysis on The Unjournal’s pipeline and evaluation output\nIn large part:\nWe may expand this analysis further in the future, e.g., to include\nThis resource aims to be:\n20 May 2024: David Reinstein, Julia Bottesini, and David Hugh-Jones have done most of the analysis here and in the accompanying dashboards.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "Colophon",
    "text": "Colophon\nThis is a Quarto book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/evaluation-data-analysis.html",
    "href": "chapters/evaluation-data-analysis.html",
    "title": "1  Descriptive data on our evaluations",
    "section": "",
    "text": "1.1 About our papers\nThis section shows the papers and/or projects we have evaluated so far.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive data on our evaluations</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation-data-analysis.html#about-our-papers",
    "href": "chapters/evaluation-data-analysis.html#about-our-papers",
    "title": "1  Descriptive data on our evaluations",
    "section": "",
    "text": "Evaluation metrics\nThe next table lists our quantitative metrics (“ratings”) for each evaluation of each paper.\n\n\n\n\n\n\n\n\n\n\nInitial pool of papers: categories\nNext, we present a plot of categories for all papers in the Unjournal’s initial pool. One paper can belong to more than one category.\n\n\n\n\n\n\n\n\n\n\n\nPaper selection\nThe Sankey diagram below starts with the papers we prioritized for likely Unjournal evaluation.\n\n\nPaper source\nThe bar plot below shows how papers came to be evaluated by us.\n\n\n\n\n\n\n\n\n\n\n\nShiny dashboard",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive data on our evaluations</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html",
    "href": "chapters/uj-data-first-look.html",
    "title": "2  A first look at Unjournal’s data",
    "section": "",
    "text": "2.1 About the data\nPapers1 can be suggested for evaluation either by Unjournal insiders, or by outsiders. The Unjournal then selects some papers for evaluation. I won’t focus on the details of this process here. Just note that we have more suggested papers than actual evaluations.\nEach paper is typically evaluated by two evaluators, though some have more or less than two. Getting two or more of every measure is useful, because it will let us check evaluations against each other.\nWe ask evaluators two kinds of quantitative questions. First, there are different measures of paper quality. Here they are, along with some snippets from our guidelines for evaluators:\nEach of these questions is meant to be a percentile scale, 0-100%, where the percentage captures the paper’s place in the distribution of the reference group (“all serious research in the same area that you have encountered in the last three years”).2 So, for example, a score of 70% would mean the paper is better than 70% of papers in the reference group. But note, the papers we evaluate are not randomly sampled from their reference group, so we should not necessarily expect them to be uniformly distributed on 0-100%.\nAs well as asking for each question (the midpoint or median of the evaluator’s belief distribution), we also ask for lower and upper bounds of a 90% credible interval.\nNext, we ask two practical questions about publication:\nTiers are measured from 0 (“won’t publish/little to no value”) up to 5 (“top journal”). Again, we ask for both an estimate and a 90% credible interval. We allow non-integer scores between 0 and 5.\nThe last question is especially interesting, because unlike all the others, it has an observable ground truth. Eventually, papers do or do not get published in specific journals, and there is often a consensus about which journals count as e.g. “top”.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html#about-the-data",
    "href": "chapters/uj-data-first-look.html#about-the-data",
    "title": "2  A first look at Unjournal’s data",
    "section": "",
    "text": "Overall assessment: “Judge the quality of the research heuristically. Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.”\nAdvancing our knowledge and practice: “To what extent does the project contribute to the field or to practice, particularly in ways that are relevant to global priorities and impactful interventions?…”\nMethods: Justification, reasonableness, validity, robustness: “Are the methods used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are the results and methods likely to be robust to reasonable changes in the underlying assumptions?…”\nLogic and communication: “Are the goals and questions of the paper clearly expressed? Are concepts clearly defined and referenced? Is the reasoning ‘transparent’? Are assumptions made explicit? Are all logical steps clear and correct? Does the writing make the argument easy to follow?”\nOpen, collaborative, replicable science: “This covers several considerations: Replicability, reproducibility, data integrity… Consistency… Useful building blocks: Do the authors provide tools, resources, data, and outputs that might enable or enhance future work and meta-analysis?”\nReal-world relevance: “Are the assumptions and setup realistic and relevant to the real world?”\nRelevance to global priorities: “Could the paper’s topic and approach potentially help inform global priorities, cause prioritization, and high-impact interventions?\n\n\n\n\n\n“What journal ranking tier should this work be published in?”\n“What journal ranking tier will this work be published in?”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html#questions-to-ask",
    "href": "chapters/uj-data-first-look.html#questions-to-ask",
    "title": "2  A first look at Unjournal’s data",
    "section": "2.2 Questions to ask",
    "text": "2.2 Questions to ask\nHere are some things we might hope to learn from our data.\n\nDo evaluators understand the questions? Do they “grok” how our percentile questions, upper bounds, and lower bounds work?\nDo evaluators take the questions seriously? Or do some of them treat them as a nuisance compared to the “real”, written review?\nBoth these questions can be partly addressed by running sanity checks. For example, do people “straightline” questions, giving the same answer for every question? Do they produce excessively narrow or wide confidence intervals?\nAre our quantitative measures accurate? Do they capture something “real” about the paper? Obviously, we don’t have access to “ground truth” – except in one case.\nAre the different measures related? Is there a single underlying dimension beneath the different numbers? Or more than one dimension?\nHow do quantitative measures relate to the written, qualitative evaluation? Does a more positive written evaluation also score higher on the numbers? Can you predict the numbers from the evaluation?\nDo evaluators understand the questions in the same way? Are different evaluators of the same paper answering the “same questions” in their head? What about evaluators of different papers in different fields?\nDo papers score differently in different fields? This could be because evaluators hold papers to different standards in those fields – or because some fields do genuinely better on some dimensions. We could ask the same question about different methodologies: for example, do randomized controlled trials score differently than other approaches?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html#sanity-checks",
    "href": "chapters/uj-data-first-look.html#sanity-checks",
    "title": "2  A first look at Unjournal’s data",
    "section": "2.3 Sanity checks",
    "text": "2.3 Sanity checks\nStraightliners are evaluators who give the same score for every question. For the midpoints, we have 0 straightliners out of 38 evaluations. We also check if people straightline lower bounds of the credible intervals (0 straightliners) and upper bounds (0 straightliners).\nEvaluators might also give “degenerate” credible intervals, with the lower bound equal to the upper bound; uninformatively wide intervals, with the lower and upper bounds equal to 0% and 100%; or simply misspecified intervals, e.g. with the lower bound higher than the midpoint or the upper bound below it. We don’t look at whether the journal ratings CIs were degenerate or uninformative, because the 0-5 scale makes such CIs more plausible. Out of 342 confidence intervals, 0 were degenerate, 0 were uninformative and 7 were misspecified.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html#accuracy",
    "href": "chapters/uj-data-first-look.html#accuracy",
    "title": "2  A first look at Unjournal’s data",
    "section": "2.4 Accuracy",
    "text": "2.4 Accuracy\nWe have no ground truth of whether a given paper scores high or low on our 7 dimensions. But because we usually have multiple evaluations per paper, we can take an indirect route. If two evaluators’ scores are correlated with reality, they will also correlate with each other. The converse does not necessarily hold: evaluators’ scores might be correlated because they both have similar prejudices or both misinterpret the paper in the same way. All the same, high “inter-rater reliability” (IRR) should increase our confidence that our scores are measuring something.\nIRR is complex. The basic form of most IRR statistics is\n\\[\n\\frac{p_a - p_e}{1 - p_e}\n\\]\nwhere \\(p_a\\) is the proportion of the time that two raters agree, and \\(p_e\\) is the amount of agreement you’d expect by chance if both raters are choosing independently.\nWhy not use \\(p_a\\) directly? Well, for example, suppose our raters pick an expected journal tier at random, from 0 to 5 inclusive. Clearly there’s no reliability: the data is just random noise. But one time in six, both raters will agree, simply by chance. So we need to adjust for the expected amount of agreement. To do this most measures use the marginal distributions of the ratings: in our example, a 1 in 6 chance of each number from 0 to 5, giving \\(p_e = 1/6\\). Krippendorff’s alpha is a widely accepted statistic that corrects for \\(p_e\\) and also defines “agreement” appropriately for different levels of measurement.\n\n\n\n\n\n\nChoosing a reliability statistic\n\n\n\nThere are many ways to measure inter-rater reliability. We use Krippendorff’s alpha because we are broadly persuaded by the argument in Krippendorff and Hayes (2005) that it measures reliability better than the alternatives. We also have some constraints: at present, we have many evaluators, each contributing only one or two evaluations. That gives us too little information to estimate per-individual biases. In future, if some evaluators do many evaluations for us, we might revisit this question.\nWe use the alpha statistic for a ratio scale, because our ratings are meant to be quantiles, which have a natural scale and zero. And we only use papers with exactly two evaluations. There is a single paper with three evaluations; adding this in would give us many missing values in the “third evaluation” column, and we’d have to use more advanced techniques to deal with these.\n\n\n\n\n\n\nTable 2.1\n\n\n\n\nKrippendorf's alpha statistics for our quantitative measures. N = 20 papers, 38 evaluations.\nDimensionKrippendorff's Alpha\n\noverall0.52 \n\nadv_knowledge0.271\n\nmethods0.134\n\nlogic_comms0.57 \n\nreal_world0.678\n\ngp_relevance0.804\n\nopen_sci0.503\n\njournal_predict0.714\n\nmerits_journal0.778\n\n\n\n\n\n\n\nBecause we have each rater’s 90% credible interval, we can also ask a slightly different question: do raters tend to agree that each other’s estimates are “reasonable”? That is, is rater 1’s midpoint estimate within rater 2’s central credible interval, and vice versa?\n\n\n\n\nTable 2.2\n\n\n\n\nProportions of midpoints within other evaluators' 90% credible intervals. N = 20 papers, 38 evaluations.\nDimensionProportion within C.I.\n\nadv_knowledge42.9%\n\nmethods46.2%\n\nlogic_comms57.1%\n\nopen_sci43.5%\n\nreal_world55.6%\n\ngp_relevance64.3%\n\njournal_predict50.0%\n\nmerits_journal36.8%\n\noverall57.1%\n\n\n\n\n\n\n\nThe table above already looks a bit worrying: typically no more than half of our evaluators’ midpoints fall within their co-evaluator’s 90% credible interval. This suggests that our evaluators may be overconfident.\n\n2.4.1 More accuracy: predicted and actual publication venues\nFor the question “What journal ranking tier will this work be published in?” evaluators give a tier from 0-5 with 5 being top, and 0 being no publication. This gives us a source of ground truth when the evaluated paper gets published.\nWhat counts as a tier 1-5 journal? Several different organizations maintain journal tier lists, and Professor Anne-Wil Harzing has collated 11 of them. Different organizations cover different journals. To get round this we impute missing data, and take the first principal component. Luckily, it turns out that this correlates highly with most of the individual rankings (Table 2.3).3\n\n\n\n\nTable 2.3: Correlations of different journal rankings and their principal component\n\n\n\n\n\nprincomp1CnrsABDCABSDenFnegeJourQual\n\nprincomp11    0.8490.79 0.8960.68 0.9340.86 \n\nCnrs0.8491    0.6380.6810.5090.8250.698\n\nABDC0.79 0.6381    0.6910.4650.7020.589\n\nABS0.8960.6810.6911    0.5680.8430.727\n\nDen0.68 0.5090.4650.5681    0.6050.526\n\nFnege0.9340.8250.7020.8430.6051    0.78 \n\nJourQual0.86 0.6980.5890.7270.5260.78 1    \n\n\n\n\n\n\n\nThe exception is the “Den” ranking (from the Danish Ministry). We checked whether “Den” added any power beyond the first principal component to predict citations or h-index data, and it didn’t.\nTo calibrate the principal component, we ran a series of ordinal logistic models predicting the different tier lists. We used just the “JourQual”, “Cnrs”, “ABS” and “Fnege” rankings, since these all had 5 tiers. Category cutoffs looked similar across all 4 rankings, so we simply took the averages to create our own meta-tier list. Figure 2.1 shows the resulting histogram of journals.\n\n\n\n\n\n\n\n\nFigure 2.1: Histogram of journal rankings by the Unjournal Tier List",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html#relatedness-and-dimensionality",
    "href": "chapters/uj-data-first-look.html#relatedness-and-dimensionality",
    "title": "2  A first look at Unjournal’s data",
    "section": "2.5 Relatedness and dimensionality",
    "text": "2.5 Relatedness and dimensionality\nWe have 7 questions measuring paper quality, and 2 questions about journal tier. We can perform a simple principal components analysis of these 9 questions. Table 2.4 shows loadings for the first three components\n\n\n\n\nTable 2.4: Loadings of first 3 principal components on ratings\n\n\n\n\n\nQuestionComp.1Comp.2Comp.3\n\noverall0.056 0.4220.117 \n\nadv_knowledge0.44  0.188−0.0999\n\nmethods0.07820.449−0.11  \n\nlogic_comms0.09340.4270.32  \n\nreal_world0.58  0.1170.0552\n\ngp_relevance0.596 −0.2010.11  \n\nopen_sci−0.176 0.3360.598 \n\njournal_predict−0.06  0.454−0.695 \n\nmerits_journal−0.249 0.1730.0721",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html#footnotes",
    "href": "chapters/uj-data-first-look.html#footnotes",
    "title": "2  A first look at Unjournal’s data",
    "section": "",
    "text": "Actually we don’t just evaluate academic papers, but I’ll use “papers” for short.↩︎\nThis ‘reference group percentile’ interpretation was introduced around the end of 2023; before this evaluators were given the description of the ratings interval seen here.↩︎\nThe calibration code is available from our Github repository.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  }
]