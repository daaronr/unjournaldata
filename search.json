[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "",
    "text": "Preface\nThe Unjournal coordinates the public evaluation of hosted papers and dynamically-presented research projects. We are working independently of traditional academic journals to build an open platform and a sustainable system for feedback, ratings, and assessment. Our initial focus is quantitative work that informs global priorities, especially in economics, policy, and other social sciences. We will encourage better research by making it easier for researchers to get feedback and credible ratings on their work. Our aim: to make rigorous research more impactful, and impactful research more rigorous.\nOur main web site, unjournal.org as well as our knowledge base, explain and present our vision, procedures, and our progress. The ‘output’ evaluations and author (including feedback and discussion) can be found on our PubPub page, and are indexed in scholarly archives.\nAlso see our interactive dashboards\nThis site and the accompanying dashboards present data and analysis on The Unjournal’s pipeline and evaluation output\nIn large part:\nWe may expand this analysis further in the future, e.g., to include\nThis resource aims to be:\n20 May 2024: David Reinstein, Julia Bottesini, and David Hugh-Jones have done most of the analysis here and in the accompanying dashboards.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "Colophon",
    "text": "Colophon\nThis is a Quarto book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/evaluation_data_input.html",
    "href": "chapters/evaluation_data_input.html",
    "title": "1  Evaluation data: input/features",
    "section": "",
    "text": "Reconcile uncertainty ratings and CIs\nWhere people gave only confidence level ‘dots’, we impute CIs (confidence/credible intervals). We follow the correspondence described here. (Otherwise, where they gave actual CIs, we use these.)1\nWe cannot publicly share the ‘papers under consideration’, but we can share some of the statistics on these papers. Let’s generate an ID (or later, salted hash) for each such paper, and keep only the shareable features of interest",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Evaluation data: input/features</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation_data_input.html#footnotes",
    "href": "chapters/evaluation_data_input.html#footnotes",
    "title": "1  Evaluation data: input/features",
    "section": "",
    "text": "Note this is only a first-pass; a more sophisticated approach may be warranted in future.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Evaluation data: input/features</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html",
    "href": "chapters/evaluation_data_analysis.html",
    "title": "2  Evaluation data: description, exploration, checks",
    "section": "",
    "text": "2.1 Data input, cleaning, feature construction and imputation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluation data: description, exploration, checks</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#data-input-cleaning-feature-construction-and-imputation",
    "href": "chapters/evaluation_data_analysis.html#data-input-cleaning-feature-construction-and-imputation",
    "title": "2  Evaluation data: description, exploration, checks",
    "section": "",
    "text": "Note on data input (10-Aug-23)\n\n\n\n\n\nBelow, the evaluation data is input from an Airtable, which itself was largely hand-input from evaluators’ reports. As PubPub builds (target: end of Sept. 2023), this will allow us to include the ratings and predictions as structured data objects. We then plan to access and input this data directly from the PubPub (API?) into the present analysis. This will improve automation and limit the potential for data entry errors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluation data: description, exploration, checks</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#basic-presentation",
    "href": "chapters/evaluation_data_analysis.html#basic-presentation",
    "title": "2  Evaluation data: description, exploration, checks",
    "section": "2.2 Basic presentation",
    "text": "2.2 Basic presentation\n\nWhat sorts of papers/projects are we considering and evaluating?\nIn this section, we give some simple data summaries and visualizations, for a broad description of The Unjournal’s coverage.\nIn the interactive table below we give some key attributes of the papers and the evaluators.\n\n\n\n\n\n\n\n\n\n\nEvaluation metrics (ratings)\nNext, a preview of the evaluations, focusing on the ‘middle ratings and predictions’:\n\n\n\n\n\n\n\n\n \n\n\n\nInitial pool of papers: categories\nNext, we present a plot of categories for all papers in the Unjournal’s initial pool. One paper can belong to more than one category.\n\n\nError in `vec_assign()` at tidyr/R/replace_na.R:74:7:\n! Can't convert `replace$category` &lt;character&gt; to match type of `data$category` &lt;logical&gt;.\n\n\n\n\n\n\n\n\nNext consider…\n\n\n\n\n\n\nComposition of research evaluated\n\nBy field (economics, psychology, etc.)\nBy subfield of economics\nBy topic/cause area (Global health, economic development, impact of technology, global catastrophic risks, etc. )\nBy source (submitted, identified with author permission, direct evaluation)\n\nTiming of intake and evaluation1\n\n\n\n\n\n\nPaper selection\nThe Sankey diagram below starts with the papers we prioritized for likely Unjournal evaluation:2.\n\n\nError in `tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 25: Existing data.\n• Size 18: Column `color`.\nℹ Only values of size one are recycled.\n\n\nError in `left_join()` at dplyr/R/rename.R:64:3:\n! Join columns in `y` must be present in the data.\n✖ Problem with `label`.\n\n\n\n\n\n\nTodo: 3\n\n\nPaper categories\n\n\n\n\n\n\n\n\n\n\n\nPaper source",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluation data: description, exploration, checks</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#the-distribution-of-ratings-and-predictions",
    "href": "chapters/evaluation_data_analysis.html#the-distribution-of-ratings-and-predictions",
    "title": "2  Evaluation data: description, exploration, checks",
    "section": "The distribution of ratings and predictions",
    "text": "The distribution of ratings and predictions\nNext, we present the ratings and predictions along with ‘uncertainty measures’.4 Where evaluators gave only a 1-5 confidence level5, we use the imputations discussed and coded above.\nBelow we present these, for each category and prediction (overall and by paper). Papers are arranged in descending order of average overall score. Note that evaluations made by anonymous reviewers are marked with a black border around the rating.\n\n\n\nError: object 'overall_lb_imp' not found\n\n\n\nBelow, we are building an interactive dashboard.6\n\nShiny dashboard\n\n\n \n\n\n\n\n\n\n\n\n\n\nNotes, clarifications, and caveats on the above dashboard\n\n\n\n\n\n\nThe aggregated ratings and ranges seem to not yet be computed properly\nIn the ‘journal ratings’ view, the stars/asterisks are used when the ‘predicted’ and ‘merited’ ratings are the same\n\n\n\n\nYou can see this dashboard on it’s own hosted here.\n\n\n\n\n\n\n\nFuture vis\n\n\n\n\n\nEach rating is a dimension or attribute (potentially normalized) potentially superimpose a ‘circle’ for the suggested weighting or overall.\nEach paper gets its own spider, with all others (or the average) in faded color behind it as a comparator.\nIdeally user can switch on/off\nBeware – people may infer things from the shape’s size\n\n\n\n\n\n\n\n\nSources of variation\nNext, look for systematic variation in the ratings\n\nBy field and topic area of paper\nBy submission/selection route\nBy evaluation manager (or their seniority, or whether they are US/Commonwealth/Other)7\n\n… perhaps building a model of this. We are looking for systematic ‘biases and trends’, loosely speaking, to help us better understand how our evaluation system is working.\n\n\n\n\nRelationship among the ratings (and predictions)\n\n\n\n\n\n\nNext steps (suggested analyses)\n\n\n\n\n\n\nCorrelation matrix\nANOVA\nPCA (Principle components)\nWith other ‘control’ factors?\nHow do the specific measures predict the aggregate ones (overall rating, merited publication)\n\nCF ‘our suggested weighting’\n\n\n\n\n\nNext chapter (analysis): aggregation of evaluator judgment\n\n\n\n\n\n\nScoping our future coverage\n\n\n\n\n\nWe have funding to evaluate roughly 50-70 papers/projects per year, given our proposed incentives.\nConsider:\n\nHow many relevant NBER papers come out per year?\nHow much relevant work in other prestige archives?\nWhat quotas do we want (by cause, etc.) and how feasible are these?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluation data: description, exploration, checks</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#footnotes",
    "href": "chapters/evaluation_data_analysis.html#footnotes",
    "title": "2  Evaluation data: description, exploration, checks",
    "section": "",
    "text": "Consider: timing might be its own section or chapter; this is a major thing journals track, and we want to keep track of ourselves↩︎\nThose marked as ‘considering’ in the Airtable↩︎\nMake interactive/dashboards of the elements below↩︎\nWe use “ub imp” (and “lb imp”) to denote the upper and lower bounds given by evaluators.↩︎\nMore or less, the ones who report a level for ‘conf overall’, although some people did this for some but not others↩︎\nWe are working to enable a range of presentations, aggregations, and analyses (your suggestions are welcome), including reasonable approaches to incorporating evaluator uncertainty↩︎\nDR: My theory is that people in commonwealth countries target a 70+ as ‘strong’ (because of their marking system) and that may drive a bias.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Evaluation data: description, exploration, checks</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html",
    "href": "chapters/uj-data-first-look.html",
    "title": "3  A first look at Unjournal’s data",
    "section": "",
    "text": "4 About the data\nPapers1 can be suggested for evaluation either by Unjournal insiders, or by outsiders. The Unjournal then selects some papers for evaluation. I won’t focus on the details of this process here. Just note that we have more suggested papers than actual evaluations.\nEach paper is typically evaluated by two evaluators, though some have more or less than two. Getting two or more of every measure is useful, because it will let us check evaluations against each other.\nWe ask evaluators two kinds of quantitative questions. First, there are different measures of paper quality. Here they are, along with some snippets from our guidelines for evaluators:\nEach of these questions is meant to be a percentile scale, 0-100%, where the percentage captures the paper’s place in the distribution of the reference group (“all serious research in the same area that you have encountered in the last three years”).2 So, for example, a score of 70% would mean the paper is better than 70% of papers in the reference group. But note, the papers we evaluate are not randomly sampled from their reference group, so we should not necessarily expect them to be uniformly distributed on 0-100%.\nAs well as asking for each question (the midpoint or median of the evaluator’s belief distribution), we also ask for lower and upper bounds of a 90% credible interval.\nNext, we ask two practical questions about publication:\nTiers are measured from 0 (“won’t publish/little to no value”) up to 5 (“top journal”). Again, we ask for both an estimate and a 90% credible interval. We allow non-integer scores between 0 and 5.\nThe last question is especially interesting, because unlike all the others, it has an observable ground truth. Eventually, papers do or do not get published in specific journals, and there is often a consensus about which journals count as e.g. “top”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  },
  {
    "objectID": "chapters/uj-data-first-look.html#footnotes",
    "href": "chapters/uj-data-first-look.html#footnotes",
    "title": "3  A first look at Unjournal’s data",
    "section": "",
    "text": "Actually we don’t just evaluate academic papers, but I’ll use “papers” for short.↩︎\nThis ‘reference group percentile’ interpretation was introduced around the end of 2023; before this evaluators were given the description of the ratings interval seen here.↩︎\nThe calibration code is available from our Github repository.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A first look at Unjournal's data</span>"
    ]
  }
]