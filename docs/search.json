[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\nCodeoptions(knitr.duplicate.label = \"allow\")"
  },
  {
    "objectID": "chapters/evaluation_data.html",
    "href": "chapters/evaluation_data.html",
    "title": "\n1  Evaluation data: description, exploration, checks\n",
    "section": "",
    "text": "load packagessource(here::here(\"code\", \"shared_packages_code.R\"))\n\n#devtools::install_github(\"rethinkpriorities/rp-r-package\")\nlibrary(rethinkpriorities)\n\n#devtools::install_github(\"rethinkpriorities/r-noodling-package\") #mainly used playing in real time\nlibrary(rnoodling)\n\nlibrary(here)\nlibrary(dplyr)\nlibrary(pacman)\n\np_load(formattable, sparkline, install=FALSE)\n\np_load(DT, santoku, lme4, huxtable, janitor, emmeans, sjPlot, sjmisc, ggeffects, ggrepel, likert, labelled, plotly, stringr, install=FALSE)\n\np_load(ggthemes, paletteer, ggridges, install=FALSE)\n\nselect <- dplyr::select \n\noptions(knitr.duplicate.label = \"allow\")\n\noptions(mc.cores = parallel::detectCores())\n#rstan_options(auto_write = TRUE)\n\n#library(hunspell)\n\n#(brms)\n\n#devtools::install_github(\"bergant/airtabler\")\np_load(airtabler)\n\n#remotes::install_github(\"rmcelreath/rethinking\")\n#library(rethinking)"
  },
  {
    "objectID": "chapters/evaluation_data.html#simple-data-summariescodebooksdashboards-and-visualization",
    "href": "chapters/evaluation_data.html#simple-data-summariescodebooksdashboards-and-visualization",
    "title": "\n1  Evaluation data: description, exploration, checks\n",
    "section": "\n2.1 Simple data summaries/codebooks/dashboards and visualization",
    "text": "2.1 Simple data summaries/codebooks/dashboards and visualization\nBelow, we give a data table of key attributes of the paper, the author, and the ‘middle’ ratings and predictions.\n\nData datable (all shareable relevant data)(\n  all_evals_dt <- evals_pub %>%\n  arrange(crucial_rsx, eval_name) %>%\n  dplyr::select(crucial_rsx, eval_name, everything())) %>%\n  dplyr::select(-id) %>% \n    dplyr::select(-matches(\"ub_|lb_|conf\")) %>% \n    #rename_all(~ gsub(\"_\", \" \", .)) %>% \n    rename(\"Research  _____________________\" = \"crucial_rsx\" \n      ) %>%\n  DT::datatable(\n    caption = \"Evaluations (confidence bounds not shown)\", \n    filter = 'top',\n    rownames= FALSE,\n    options = list(pageLength = 7)\n    )\n\n\nNext, we present the ratings and predictions along with ‘uncertainty measures’. We use “ub imp” (and “lb imp”) to denote the upper and lower bounds given by evaluators. Where evaluators gave only a 1-5 confidence level1, we use the imputations discussed and coded above.\n\nCode(\n  all_evals_dt_ci <- evals_pub %>%\n  arrange(crucial_rsx, eval_name) %>%\n  dplyr::select(crucial_rsx, eval_name, conf_overall, matches(\"ub_imp|lb_imp\")) %>%\n    #rename_all(~ gsub(\"_\", \" \", .)) %>% \n    rename(\"Research  _____________________\" = \"crucial_rsx\" \n      ) %>%\n  DT::datatable(\n    caption = \"Evaluations and (imputed*) confidence bounds)\", \n    filter = 'top',\n    rownames= FALSE,\n    options = list(pageLength = 7)\n    )\n)\n\n\n\nComposition of research evaluated\n\nBy field (economics, psychology, etc.)\nBy subfield of economics\nBy topic/cause area (Global health, economic development, impact of technology, global catastrophic risks, etc. )\nBy source (submitted, identified with author permission, direct evaluation)\n\n\nTiming of intake and evaluation (Consider: timing might be its own section or chapter; this is a major thing journals track, and we want to keep track of ourselves)\n\n\nCode#Add in the 3 different evaluation input sources\n#update to be automated rather than hard-coded - to look at David's work here\npapers_considered = all_pub_records %>%nrow()\npapers_deprio = all_pub_records %>% filter(`stage of process/todo` ==  \"de-prioritized\") %>%nrow()\npapers_evaluated = all_pub_records %>% filter(`stage of process/todo` %in%  c(\"published\",\n                                                                              \"contacting/awaiting_authors_response_to_evaluation\",\n                                                                               \"awaiting_publication_ME_comments\",\n                                                                              \"awaiting_evaluations\")) %>%nrow()\npapers_complete = all_pub_records %>% filter(`stage of process/todo` ==  \"published\") %>% nrow()\npapers_in_progress = papers_evaluated-papers_complete\npapers_still_in_consideration = all_pub_records %>% filter(`stage of process/todo` ==  \"considering\") %>%nrow()\n\n\nfig <- plot_ly(\n    type = \"sankey\",\n    orientation = \"h\",\n\n    node = list(\n      label = c(\"All paper considered\", \"Papers evaluated\", \"Papers complete\", \"Papers in progress\", \"Papers still in consideration\", \"Papers rejected\"),\n      color = c(\"orange\", \"green\", \"green\", \"orange\", \"orange\", \"red\"),\n      pad = 15,\n      thickness = 20,\n      line = list(\n        color = \"black\",\n        width = 0.5\n      )\n    ),\n\n    link = list(\n      source = c(0,1,1,0,0),\n      target = c(1,2,3,4,5),\n      value =  c(\n        papers_evaluated,\n        papers_complete,\n        papers_in_progress,\n        papers_still_in_consideration,\n        papers_deprio\n    ))\n  )\nfig <- fig %>% layout(\n    title = \"Unjournal paper funnel\",\n    font = list(\n      size = 10\n    )\n)\n\nfig \n\n\n\n\n\nThe distribution of ratings and predictions\n\nFor each category and prediction (overall and by paper)\n\n\nCodesummary_df <- evals_pub %>%\n  distinct(crucial_research_unlisted, .keep_all = T) %>% \n  group_by(category_unlisted) %>%\n  summarise(count = n()) \n\nsummary_df$category_unlisted[is.na(summary_df$category_unlisted)] <- \"Unknown\"\n\nsummary_df <- summary_df%>%\n  arrange(-desc(count)) %>%\n  mutate(category_unlisted = factor(category_unlisted, levels = unique(category_unlisted)))\n\n# Create stacked bar chart\nggplot(summary_df, aes(x = category_unlisted, y = count)) +\n  geom_bar(stat = \"identity\") + \n  coord_flip() + # This makes the chart horizontal\n  theme_minimal() +\n  labs(x = \"Paper category\", y = \"Count\", \n       title = \"Count of evaluated papaers by category\") \n\n\n\n\n\nCodewrap_text <- function(text, width) {\n  sapply(strwrap(text, width = width, simplify = FALSE), paste, collapse = \"\\n\")\n}\n\nevals_pub$wrapped_pub_names <- wrap_text(evals_pub$crucial_research_unlisted, width = 60)\n\n\n# original names\noriginal_names <- evals_pub$crucial_research_unlisted\n\n# shortened names\nshortened_names <- c(\"Resilient Foods vs AGI Safety\",\n                     \"Advance Market Commitments\",\n                     \"Wildlife Trade Demand\",\n                     \"Advance Market Commitments\",\n                     \"Economic Prod. & Biodiversity\",\n                     \"Advance Market Commitments\",\n                     \"AI and Economic Growth\",\n                     \"Wildlife Trade Demand\",\n                     \"Non-Profits Governance & Impact\",\n                     \"AI and Economic Growth\",\n                     \"Mental Health Therapy & Human Capital\",\n                     \"Economic Prod. & Biodiversity\",\n                     \"Cash Transfers vs Psychotherapy\",\n                     \"Cash Transfers vs Psychotherapy\",\n                     \"Resilient Foods vs AGI Safety\",\n                     \"Mental Health Therapy & Human Capital\",\n                     \"Resilient Foods vs AGI Safety\")\n\n# create a named vector for easy lookup\nname_lookup <- setNames(shortened_names, original_names)\n\n# use the lookup to create the new column\nevals_pub$shortened_names <- name_lookup[evals_pub$crucial_research_unlisted]\n\nevals_pub$wrapped_shortened_names <- wrap_text(evals_pub$shortened_names, width = 15)\n\n#Move this to do this 'cleaning' earlier\nevals_pub$revised_evaluator_name <- ifelse(\n  grepl(\"^\\\\b\\\\w+\\\\b$|\\\\bAnonymous\\\\b\", evals_pub$evaluator_name_unlisted),\n  paste0(\"Anonymous_\", seq_along(evals_pub$evaluator_name_unlisted)),\n  evals_pub$evaluator_name_unlisted\n)\n\n\n# Dot plot\nggplot(evals_pub, aes(x = shortened_names, y = overall)) +\n  geom_point(stat = \"identity\", size = 4, shape = 1, colour = \"lightblue\", stroke = 3) +\n  geom_text_repel(aes(label = revised_evaluator_name), \n                  size = 3, \n                  box.padding = unit(0.35, \"lines\"),\n                  point.padding = unit(0.3, \"lines\")) +\n  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)\n  theme_light() +\n  xlab(\"Paper\") + # remove x-axis label\n  ylab(\"Overall score\") + # name y-axis\n  ggtitle(\"Overall scores of evaluated papers\") +# add title\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    text = element_text(size = 14), # changing all text size to 16\n    axis.text.y = element_text(size = 10),\n    axis.text.x = element_text(size = 12)\n  )\n\n\n\n\n\nCodeggplot(evals_pub, aes(x = source_main)) + \n  geom_bar(position = \"stack\", stat = \"count\") +\n  labs(x = \"Source\", y = \"Count\") +\n  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)\n  theme_light() +\n  theme_minimal() +\n  ggtitle(\"Evaluations by source of the paper\") +# add title\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    text = element_text(size = 16), # changing all text size to 16\n    axis.text.y = element_text(size = 14),\n    axis.text.x = element_text(size = 14)\n    )\n\n\n\n\n\nCodeall_pub_records$is_evaluated = all_pub_records$`stage of process/todo` %in%  c(\"published\",\n                                                                              \"contacting/awaiting_authors_response_to_evaluation\",\n                                                                               \"awaiting_publication_ME_comments\",\n                                                                              \"awaiting_evaluations\")\n\nall_pub_records$source_main[all_pub_records$source_main == \"NA\"] <- \"Not applicable\"  \nall_pub_records$source_main = tidyr::replace_na(all_pub_records$source_main, \"Unknown\")\n\nggplot(all_pub_records, aes(x = fct_infreq(source_main), fill = is_evaluated)) + \n  geom_bar(position = \"stack\", stat = \"count\") +\n  labs(x = \"Source\", y = \"Count\", fill = \"Selected for\\nevaluation?\") +\n  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)\n  theme_light() +\n  theme_minimal() +\n  ggtitle(\"Evaluations by source of the paper\") +# add title\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    text = element_text(size = 16), # changing all text size to 16\n    axis.text.y = element_text(size = 14),\n    axis.text.x = element_text(size = 14)\n    )\n\n\n\n\n\nCodeunit.scale = function(x) (x*100 - min(x*100)) / (max(x*100) - min(x*100))\nevaluations_table <- evals_pub %>%\n  select(crucial_rsx, eval_name, cat_1, source_main, overall, adv_knowledge, methods, logic_comms, journal_predict) %>%\n  arrange(desc(crucial_rsx))\n\n\nout = formattable(\n  evaluations_table,\n  list(\n    #area(col = 5:8) ~ function(x) percent(x / 100, digits = 0),\n    area(col = 5:8) ~ color_tile(\"#FA614B66\",\"#3E7DCC\"),\n    `journal_predict` = proportion_bar(\"#DeF7E9\", unit.scale)\n  )\n)\nout\n\n\n\n\n\ncrucial_rsx\n\n\neval_name\n\n\ncat_1\n\n\nsource_main\n\n\noverall\n\n\nadv_knowledge\n\n\nmethods\n\n\nlogic_comms\n\n\njournal_predict\n\n\n\n\n\nThe Governance Of Non-Profits And Their Social Impact: Evidence From A Randomized Program In Healthcare In DRC\n\n\n***REMOVED***\n\n\nGH&D\n\n\ninternal-NBER\n\n\n65\n\n\n70\n\n\n60\n\n\n55\n\n\n3.6\n\n\n\n\nThe Comparative Impact of Cash Transfers and a Psychotherapy Program on Psychological and Economic Well-being\n\n\nAnonymous Reviewer 1\n\n\nGH&D\n\n\ninternal-NBER\n\n\n90\n\n\n90\n\n\n90\n\n\n80\n\n\n4.0\n\n\n\n\nThe Comparative Impact of Cash Transfers and a Psychotherapy Program on Psychological and Economic Well-being\n\n\nHannah Metzler\n\n\nGH&D\n\n\ninternal-NBER\n\n\n75\n\n\n70\n\n\n90\n\n\n75\n\n\n3.0\n\n\n\n\nMental Health Therapy as a Core Strategy for Increasing Human Capital: Evidence from Ghana (renamed “Cognitive Behavioral Therapy among Ghana’s Rural Poor Is Effective Regardless of Baseline Mental Distress”)\n\n\nb62275b05d45f43cce4e494d31a07c19\n\n\nNA\n\n\ninternal-NBER\n\n\n75\n\n\n60\n\n\n90\n\n\n70\n\n\n4.0\n\n\n\n\nMental Health Therapy as a Core Strategy for Increasing Human Capital: Evidence from Ghana (renamed “Cognitive Behavioral Therapy among Ghana’s Rural Poor Is Effective Regardless of Baseline Mental Distress”)\n\n\n47273de4862aaff608f9086d4d643054\n\n\nNA\n\n\ninternal-NBER\n\n\n75\n\n\n65\n\n\n60\n\n\n75\n\n\nNA\n\n\n\n\nLong term cost-effectiveness of resilient foods for global catastrophes compared to artificial general intelligence safety (Denkenberger et al)\n\n\nScott Janzwood\n\n\nlong-term-relevant\n\n\nsubmitted\n\n\n65\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\nLong term cost-effectiveness of resilient foods for global catastrophes compared to artificial general intelligence safety (Denkenberger et al)\n\n\nAnca Hanea\n\n\nlong-term-relevant\n\n\nsubmitted\n\n\n80\n\n\n80\n\n\n70\n\n\n85\n\n\n3.5\n\n\n\n\nLong term cost-effectiveness of resilient foods for global catastrophes compared to artificial general intelligence safety (Denkenberger et al)\n\n\nAlex Bates\n\n\nlong-term-relevant\n\n\nsubmitted\n\n\n40\n\n\n30\n\n\n50\n\n\n60\n\n\n2.0\n\n\n\n\nKremer, M., Levin, J. and Snyder, C.M., 2020, May. Advance Market Commitments: Insights from Theory and Experience. In AEA Papers and Proceedings (Vol. 110, pp. 269-73).\n\n\nDavid Manheim\n\n\npolicy\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n80\n\n\n25\n\n\n95\n\n\n75\n\n\n3.0\n\n\n\n\nKremer, M., Levin, J. and Snyder, C.M., 2020, May. Advance Market Commitments: Insights from Theory and Experience. In AEA Papers and Proceedings (Vol. 110, pp. 269-73).\n\n\nJoel Tan\n\n\npolicy\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n79\n\n\n90\n\n\n70\n\n\n70\n\n\n5.0\n\n\n\n\nKremer, M., Levin, J. and Snyder, C.M., 2020, May. Advance Market Commitments: Insights from Theory and Experience. In AEA Papers and Proceedings (Vol. 110, pp. 269-73).\n\n\nDan Tortorice\n\n\npolicy\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n80\n\n\n90\n\n\n80\n\n\n80\n\n\n4.0\n\n\n\n\nBanning wildlife trade can boost demand for unregulated threatened species\n\n\nAnonymous\n\n\nconservation\n\n\nsubmitted\n\n\n75\n\n\n70\n\n\n80\n\n\n70\n\n\n3.0\n\n\n\n\nBanning wildlife trade can boost demand for unregulated threatened species\n\n\nLiew Jia Huan\n\n\nconservation\n\n\nsubmitted\n\n\n75\n\n\n80\n\n\n50\n\n\n70\n\n\n2.5\n\n\n\n\nAghion, P., Jones, B.F., and Jones, C.I., 2017. Artificial Intelligence and Economic Growth\n\n\nPhil Trammel\n\n\nmacroeconomics\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n92\n\n\n97\n\n\n70\n\n\n45\n\n\n3.5\n\n\n\n\nAghion, P., Jones, B.F., and Jones, C.I., 2017. Artificial Intelligence and Economic Growth\n\n\nSeth Benzell\n\n\nmacroeconomics\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n80\n\n\n75\n\n\n80\n\n\n70\n\n\nNA\n\n\n\n\n“The Environmental Effects of Economic Production: Evidence from Ecological Observations (previous title: Economic Production and Biodiversity in the United States)”\n\n\nElias Cisneros\n\n\nNA\n\n\ninternal-NBER\n\n\n88\n\n\n90\n\n\n75\n\n\n80\n\n\n4.0\n\n\n\n\n“The Environmental Effects of Economic Production: Evidence from Ecological Observations (previous title: Economic Production and Biodiversity in the United States)”\n\n\n1ef6aff67012a1750f88f631fddb346c\n\n\nNA\n\n\ninternal-NBER\n\n\n70\n\n\n70\n\n\n70\n\n\n75\n\n\n4.0\n\n\n\n\n\n\n\nBy field and topic area of paper\nBy submission/selection route\nBy evaluation manager\n\n\nRelationship among the ratings (and predictions)\n\nCorrelation matrix\nANOVA\nPCA (Principle components)\nWith other ‘control’ factors?\n\nHow do the specific measures predict the aggregate ones (overall rating, merited publication)\n\nCF ‘our suggested weighting’"
  },
  {
    "objectID": "chapters/evaluation_data.html#aggregation-of-expert-opinion-modeling",
    "href": "chapters/evaluation_data.html#aggregation-of-expert-opinion-modeling",
    "title": "\n1  Evaluation data: description, exploration, checks\n",
    "section": "\n2.2 Aggregation of expert opinion (modeling)",
    "text": "2.2 Aggregation of expert opinion (modeling)"
  },
  {
    "objectID": "chapters/evaluation_data.html#notes-on-sources-and-approaches",
    "href": "chapters/evaluation_data.html#notes-on-sources-and-approaches",
    "title": "\n1  Evaluation data: description, exploration, checks\n",
    "section": "\n2.3 Notes on sources and approaches",
    "text": "2.3 Notes on sources and approaches\n\n\n\n\n\n\nHanea et al\n\n\n\n\n\n(Consult, e.g., repliCATS/Hanea and others work; meta-science and meta-analysis approaches)\naggrecat package\n\nAlthough the accuracy, calibration, and informativeness of the majority of methods are very similar, a couple of the aggregation methods consistently distinguish themselves as among the best or worst. Moreover, the majority of methods outperform the usual benchmarks provided by the simple average or the median of estimates.\n\nHanea et al, 2021\nHowever, these are in a different context. Most of those measures are designed to deal with probablistic forecasts for binary outcomes, where the predictor also gives a ‘lower bound’ and ‘upper bound’ for that probability. We could roughly compare that to our continuous metrics with 90% CI’s (or imputations for these).\nFurthermore, many (all their successful measures?) use ‘performance-based weights’, accessing metrics from prior prediction performance of the same forecasters We do not have these, nor do we have a sensible proxy for this.\n\n\n\n\n2.3.1 D Veen et al (2017)\nlink\n… we show how experts can be ranked based on their knowledge and their level of (un)certainty. By letting experts specify their knowledge in the form of a probability distribution, we can assess how accurately they can predict new data, and how appropriate their level of (un)certainty is. The expert’s specified probability distribution can be seen as a prior in a Bayesian statistical setting. We evaluate these priors by extending an existing prior-data (dis)agreement measure, the Data Agreement Criterion, and compare this approach to using Bayes factors to assess prior specification. We compare experts with each other and the data to evaluate their appropriateness. Using this method, new research questions can be asked and answered, for instance: Which expert predicts the new data best? Is there agreement between my experts and the data? Which experts’ representation is more valid or useful? Can we reach convergence between expert judgement and data? We provided an empirical example ranking (regional) directors of a large financial institution based on their predictions of turnover.\nBe sure to consult the correction made here\n:::\n\n\n\n\n\n\nAlso seems relevant:\n\n\n\n\n\nSee Gsheet HERE, generated from an Elicit.org inquiry.\n\n\n\nIn spite of the caveats in the fold above, we construct some measures of aggregate beliefs using the aggrecat package. We will make (and explain) some ad-hoc choices here. We present these:\n\nFor each paper\nFor categories of papers and cross-paper categories of evaluations\nFor the overall set of papers and evaluations\n\nWe can also hold onto these aggregated metrics for later use in modeling.\n\nSimple averaging\nBayesian approaches\nBest-performing approaches from elsewhere\nAssumptions over unit-level random terms\nExplicit modeling of ‘research quality’ (for use in prizes, etc.)\n\nUse the above aggregation as the outcome of interest, or weight towards categories of greater interest?\nModel with controls – look for greatest positive residual?"
  },
  {
    "objectID": "chapters/evaluation_data.html#inter-rater-reliability",
    "href": "chapters/evaluation_data.html#inter-rater-reliability",
    "title": "\n1  Evaluation data: description, exploration, checks\n",
    "section": "\n2.4 Inter-rater reliability",
    "text": "2.4 Inter-rater reliability"
  },
  {
    "objectID": "chapters/evaluation_data.html#decomposing-variation-dimension-reduction-simple-linear-models",
    "href": "chapters/evaluation_data.html#decomposing-variation-dimension-reduction-simple-linear-models",
    "title": "\n1  Evaluation data: description, exploration, checks\n",
    "section": "\n2.5 Decomposing variation, dimension reduction, simple linear models",
    "text": "2.5 Decomposing variation, dimension reduction, simple linear models"
  },
  {
    "objectID": "chapters/evaluation_data.html#later-possiblities",
    "href": "chapters/evaluation_data.html#later-possiblities",
    "title": "\n1  Evaluation data: description, exploration, checks\n",
    "section": "\n2.6 Later possiblities",
    "text": "2.6 Later possiblities\n\nRelation to evaluation text content (NLP?)\nRelation/prediction of later outcomes (traditional publication, citations, replication)"
  },
  {
    "objectID": "chapters/evaluation_data.html#scoping-our-future-coverage",
    "href": "chapters/evaluation_data.html#scoping-our-future-coverage",
    "title": "\n1  Evaluation data: description, exploration, checks\n",
    "section": "\n2.7 Scoping our future coverage",
    "text": "2.7 Scoping our future coverage\nWe have funding to evaluate roughly 50-70 papers/projects per year, given our proposed incentives.\nConsider:\n\nHow many relevant NBER papers come out per year?\nHow much relevant work in other prestige archives?\nWhat quotas do we want (by cause, etc.) and how feasible are these?"
  }
]