[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "",
    "text": "Preface\nThe Unjournal coordinates the public evaluation of hosted papers and dynamically-presented research projects. We are working independently of traditional academic journals to build an open platform and a sustainable system for feedback, ratings, and assessment. Our initial focus is quantitative work that informs global priorities, especially in economics, policy, and other social sciences. We will encourage better research by making it easier for researchers to get feedback and credible ratings on their work. Our aim: to make rigorous research more impactful, and impactful research more rigorous.\nOur main web site, unjournal.org, explains and presents our vision, procedures, and our progress. The ‘output’ evaluations and author (including feedback and discussion) can be found on our PubPub page, and are indexed in scholarly archives.\nThe present site presents data and analysis on The Unjournal’s pipeline and evaluation output\nIn large part:\nWe may expand this analysis further in the future, e.g., to include\nThis resource aims to be:\n10 Aug 2023: David Reinstein and Julia Bottesini have done most of the analysis here."
  },
  {
    "objectID": "index.html#collophon",
    "href": "index.html#collophon",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "Collophon",
    "text": "Collophon\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\nCodeoptions(knitr.duplicate.label = \"allow\")"
  },
  {
    "objectID": "chapters/evaluation_data_input.html#footnotes",
    "href": "chapters/evaluation_data_input.html#footnotes",
    "title": "\n1  Evaluation data: input/features\n",
    "section": "",
    "text": "Note this is only a first-pass; a more sophisticated approach may be warranted in future.↩︎"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#data-input-cleaning-feature-construction-and-imputation",
    "href": "chapters/evaluation_data_analysis.html#data-input-cleaning-feature-construction-and-imputation",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.1 Data input, cleaning, feature construction and imputation",
    "text": "2.1 Data input, cleaning, feature construction and imputation\n\nload packageslibrary(tidyverse) \n\n# markdown et al. ----\nlibrary(knitr)\nlibrary(bookdown)\nlibrary(rmarkdown)\nlibrary(shiny)\nlibrary(quarto)\nlibrary(formattable) # Create 'Formattable' Data Structures\nlibrary(DT) # R interface to DataTables library (JavaScript)\n\n# dataviz ----\nlibrary(ggrepel)\nlibrary(plotly) # Create Interactive Web Graphics via 'plotly.js'\n\n# others ----\nlibrary(here) # A Simpler Way to Find Your Files\n# renv::install(packages = \"metamelb-repliCATS/aggreCAT\")\n#library(aggreCAT)\n\n# Make sure select is always the dplyr version\nselect &lt;- dplyr::select \n\n# options\noptions(knitr.duplicate.label = \"allow\")\noptions(mc.cores = parallel::detectCores())\n\n\n\n\n\n\n\n\nNote on data input (10-Aug-23)\n\n\n\n\n\nBelow, the evaluation data is input from an Airtable, which itself was largely hand-input from evaluators’ reports. As PubPub builds (target: end of Sept. 2023), this will allow us to include the ratings and predictions as structured data objects. We then plan to access and input this data directly from the PubPub (API?) into the present analysis. This will improve automation and limit the potential for data entry errors.\n\n\n\n\nInput evaluation dataevals_pub &lt;- readRDS(file = here(\"data\", \"evals.Rdata\"))\nall_papers_p &lt;- readRDS(file = here(\"data\", \"all_papers_p.Rdata\"))\n\n\n\nDefine lists of columns to use later# Lists of categories\nrating_cats &lt;- c(\"overall\", \"adv_knowledge\", \"methods\", \"logic_comms\", \"real_world\", \"gp_relevance\", \"open_sci\")\n\n#... 'predictions' are currently 1-5 (0-5?)\npred_cats &lt;- c(\"journal_predict\", \"merits_journal\")"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#basic-presentation",
    "href": "chapters/evaluation_data_analysis.html#basic-presentation",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.2 Basic presentation",
    "text": "2.2 Basic presentation\nWhat sorts of papers/projects are we considering and evaluating?\nIn this section, we give some simple data summaries and visualizations, for a broad description of The Unjournal’s coverage.\nIn the interactive table below we give some key attributes of the papers and the evaluators.\n\n\nCodeevals_pub_df_overview &lt;- evals_pub %&gt;%\n  arrange(paper_abbrev, eval_name) %&gt;%\n  dplyr::select(paper_abbrev, crucial_rsx, eval_name, cat_1, cat_2, source_main, author_agreement) %&gt;%\n  dplyr::select(-matches(\"ub_|lb_|conf\")) \n\nevals_pub_df_overview %&gt;%   \n   rename(\n    \"Paper Abbreviation\" = paper_abbrev,\n    \"Paper name\" = crucial_rsx,\n    \"Evaluator Name\" = eval_name,\n    \"Main category\" = cat_1,\n    \"Category 2\" = cat_2,\n    \"Main source\" = source_main,\n    \"Author contact\" = author_agreement,\n  ) %&gt;% \n  DT::datatable(\n    caption = \"Evaluations (confidence bounds not shown)\", \n    filter = 'top',\n    rownames= FALSE,\n    options = list(pageLength = 5,\n      columnDefs = list(list(width = '150px', targets = 1)))) %&gt;% \n  formatStyle(columns = 2:ncol(evals_pub_df_overview), \n              textAlign = 'center') %&gt;% \nformatStyle(\n    \"Paper name\",\n    fontSize = '10px'\n  )\n\n\n\n\n\nCoderm(evals_pub_df_overview)\n\n\n\n\nEvaluation metrics (ratings)\n\nCoderename_dtstuff &lt;- function(df){\n  df %&gt;%  \n  rename(\n    \"Paper Abbreviation\" = paper_abbrev,\n    \"Evaluator Name\" = eval_name,\n    \"Advancing knowledge\" = adv_knowledge,\n    \"Methods\" = methods,\n    \"Logic & comm.\" = logic_comms,\n    \"Real world engagement\" = real_world,\n    \"Global priorities relevance\" = gp_relevance,\n    \"Open Science\" = open_sci\n  )\n}\n\n\n\n\nCode# Need to find a way to control column width but it seems to be a problem with DT\n# https://github.com/rstudio/DT/issues/29\n\n\nevals_pub_df &lt;- evals_pub %&gt;%\n  # Arrange data\n  arrange(paper_abbrev, eval_name, overall) %&gt;%\n  \n  # Select and rename columns\n  dplyr::select(paper_abbrev, eval_name, all_of(rating_cats)) %&gt;%\n rename_dtstuff \n\n\n(\n evals_pub_dt &lt;- evals_pub_df %&gt;%  \n  # Convert to a datatable and apply styling\n  datatable(\n    caption = \"Evaluations and predictions (confidence bounds not shown)\", \n    filter = 'top',\n    rownames = FALSE,\n    options = list(pageLength = 5, \n            columnDefs = list(list(width = '150px', targets = 0)))) %&gt;% \n  formatStyle(columns = 2:ncol(evals_pub_df), \n              textAlign = 'center')\n)\n\n\n\n\n\n\n\n\nNext, a preview of the evaluations, focusing on the ‘middle ratings and predictions’:\n\n\nData datable (all shareable relevant data)# we didn't seem to be using all_evals_dt so I removed it to increase readability\n\n\nevals_pub %&gt;%\n  arrange(paper_abbrev, eval_name, overall) %&gt;%\n  dplyr::select(paper_abbrev, eval_name, all_of(rating_cats))  %&gt;%\n  rename_dtstuff %&gt;%  \n  DT::datatable(\n    caption = \"Evaluations and predictions (confidence bounds not shown)\", \n    filter = 'top',\n    rownames= FALSE,\n    options = list(pageLength = 5,\n                  columnDefs = list(list(width = '150px', targets = 0))) \n\n    )\n\n\n\n\n\n\n\n \n\n\nCode# we did not seem to be using all_evals_dt_ci so I removed it to improve readability\nevals_pub %&gt;%\n  arrange(paper_abbrev, eval_name) %&gt;%\n  dplyr::select(paper_abbrev, eval_name, conf_overall, all_of(rating_cats), matches(\"ub_imp|lb_imp\")) %&gt;%\n  rename_dtstuff %&gt;% \n  DT::datatable(\n    caption = \"Evaluations and (imputed*) confidence bounds)\", \n    filter = 'top',\n    rownames= FALSE,\n    options = list(pageLength = 5)\n    )\n\n\n\n\n\n\n\n\nNext consider…\n\n\n\n\n\n\nComposition of research evaluated\n\nBy field (economics, psychology, etc.)\nBy subfield of economics\nBy topic/cause area (Global health, economic development, impact of technology, global catastrophic risks, etc. )\nBy source (submitted, identified with author permission, direct evaluation)\n\n\nTiming of intake and evaluation1\n\n\n\n\n\nPaper selection\nThe Sankey diagram below starts with the papers we prioritized for likely Unjournal evaluation:2.\n\nCode#Add in the 3 different evaluation input sources\n#update to be automated rather than hard-coded - to look at David's work here\n\npapers_considered &lt;- all_papers_p %&gt;% \n  nrow()\n\npapers_deprio &lt;- all_papers_p %&gt;% \n  filter(`stage of process/todo` ==  \"de-prioritized\") %&gt;% \n  nrow()\n\npapers_evaluated &lt;- all_papers_p %&gt;% \n  filter(`stage of process/todo` %in% c(\"published\",\n                                        \"contacting/awaiting_authors_response_to_evaluation\",\n                                        \"awaiting_publication_ME_comments\",\"awaiting_evaluations\")) %&gt;% \n  nrow()\n\npapers_complete &lt;- all_papers_p %&gt;% \n  filter(`stage of process/todo` ==  \"published\") %&gt;%\n  nrow()\n\npapers_in_progress &lt;-  papers_evaluated - papers_complete\n\npapers_still_in_consideration &lt;-  all_papers_p %&gt;% filter(`stage of process/todo` ==  \"considering\") %&gt;% nrow()\n\n\n#todo: adjust wording of hover notes ('source, target...etc')\n\nfig &lt;- plot_ly(\n  type = \"sankey\",\n  orientation = \"h\",\n  \n  node = list(\n    label = c(\"Prioritized\", \"Evaluating\", \"Complete\", \"In progress\", \"Still in consideration\", \"De-prioritized\"),\n    color = c(\"orange\", \"green\", \"green\", \"orange\", \"orange\", \"red\"),\n    #Todo: adjust 'location' to group these left to right\n    pad = 15,\n    thickness = 20,\n    line = list(\n      color = \"black\",\n      width = 0.5\n    )\n  ),\n  \n  link = list(\n    source = c(0,1,1,0,0),\n    target = c(1,2,3,4,5),\n    value =  c(\n      papers_evaluated,\n      papers_complete,\n      papers_in_progress,\n      papers_still_in_consideration,\n      papers_deprio\n    ))\n)\nfig &lt;- fig %&gt;% layout(\n  title = \"Unjournal paper funnel\",\n  font = list(\n    size = 10\n  )\n)\n\nfig \n\n\n\n\n\nTodo: 3\nPaper categories\n\nCodeevals_pub %&gt;% \n  select(paper_abbrev, starts_with(\"cat_\")) %&gt;%\n  distinct() %&gt;% \n  pivot_longer(cols = starts_with(\"cat_\"), names_to = \"CatNum\", values_to = \"Category\") %&gt;% \n  group_by(CatNum, Category) %&gt;% \n  count() %&gt;% \n  filter(!is.na(Category)) %&gt;% \n  mutate(Category = str_to_title(Category),\n         CatNum = ordered(CatNum, \n                          levels = c(\"cat_1\", \"cat_2\", \"cat_3\"),\n                          labels = c(\"Primary\", \"Secondary\", \"Tertiary\"))) %&gt;%\n  ggplot(aes(x = reorder(Category, -n), y = n)) +\n  geom_bar(aes(fill = CatNum), stat = \"identity\", color = \"grey30\") + \n  labs(x = \"Paper category\", y = \"Count\", fill = \"Cat Level\",\n       title = \"Paper categories represented in pilot data\") +\n  theme_bw() +\n  facet_grid(~CatNum, scales=\"free_x\", space=\"free_x\") +\n  theme(axis.text.x=element_text(angle=45,hjust=1)) +\n  theme(legend.position = \"none\")\n\n\n\n\nPaper source\n\nCode# Bar plot\nevals_pub %&gt;% \n  rowwise() %&gt;% \n  mutate(source_main = str_replace_all(string = source_main, \n                                       pattern = \"-\", \n                                       replace = \" \") %&gt;% str_to_title()) %&gt;%\n  select(paper_abbrev, source_main) %&gt;% \n  distinct() %&gt;%\n  ggplot(aes(x = source_main)) + \n  geom_bar(position = \"stack\", stat = \"count\", color = \"grey30\", fill = \"grey80\") +\n  labs(x = \"Source\", y = \"Count\") +\n  labs(title = \"Pool of research/evaluations by paper source\") +\n  theme_bw() +\n  theme(text = element_text(size = 15)) +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))\n\n\n\n\n\nCode# JB: Most of these should probably be cleaned in data storage\n\nlibrary(RColorBrewer)  # for color palettes\n\n# paper statuses that are considered \"being evaluated\"\neval_true = c(\"published\", \n              \"contacting/awaiting_authors_response_to_evaluation\",\n              \"awaiting_publication_ME_comments\",\n              \"awaiting_evaluations\")\n\n# Is the paper being evaluated? \nall_papers_p &lt;- all_papers_p %&gt;% \n  mutate(is_evaluated = if_else(`stage of process/todo` %in% eval_true, TRUE, FALSE))\n\n# main source clean\nall_papers_p &lt;- all_papers_p %&gt;% \n  mutate(source_main = case_when(source_main == \"NA\" ~ \"Not applicable\",\n                                 source_main == \"internal-from-syllabus-agenda-policy-database\" ~ \"Internal: syllabus, agenda, etc.\",\n                                 is.na(source_main) ~ \"Unknown\",\n                                 TRUE ~ source_main))\n\nall_papers_p %&gt;% \nggplot(aes(x = fct_infreq(source_main), fill = is_evaluated)) + \n  geom_bar(position = \"stack\", stat = \"count\") +\n  labs(x = \"Source\", y = \"Count\", fill = \"Selected for\\nevaluation?\") +\n  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)\n  labs(title = \"Evaluations by source of the paper\") +\n  theme_bw() +\n  theme(text = element_text(size = 15)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#the-distribution-of-ratings-and-predictions",
    "href": "chapters/evaluation_data_analysis.html#the-distribution-of-ratings-and-predictions",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "The distribution of ratings and predictions",
    "text": "The distribution of ratings and predictions\nNext, we present the ratings and predictions along with ‘uncertainty measures’.4 Where evaluators gave only a 1-5 confidence level5, we use the imputations discussed and coded above.\n\nFor each category and prediction (overall and by paper)\n\n\nCode# evals_pub %&gt;% \n#   select(matches(\"overall\")) %&gt;% \n#   view()\n\n\n\n\nCode# Generate a color palette with more colors\ncolor_count &lt;- length(unique(evals_pub$paper_abbrev))\ncolor_palette &lt;- colorRampPalette(brewer.pal(8, \"Set1\"))(color_count)\n\n# set one \"set\" of dodge width values across layers\npd = position_dodge(width = 0.8)\n\n# Dot plot\ng1 &lt;- evals_pub %&gt;% \n  ggplot(aes(x = paper_abbrev, y = overall, \n             text = paste0('Evaluator: ', eval_name,   # tooltip data\n                           '&lt;br&gt;Rating [CI]: ', overall, \" [\", overall_lb_imp, \", \", overall_ub_imp, \"]\"))) +\n  geom_point(aes(color = paper_abbrev), \n             stat = \"identity\", size = 2, shape = 18, stroke = 1, \n             position = pd) +\n  geom_linerange(aes(ymin = overall_lb_imp, ymax = overall_ub_imp, color = paper_abbrev), \n                 position = pd) +\n  geom_text(data = subset(evals_pub, str_detect(eval_name, \"Anonymous\")),\n            aes(label = \"anon.\"), size=3) +\n  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)\n  labs(x = \"Paper\", y = \"Overall score\",\n       title = \"Overall scores of evaluated papers\") +\n  theme_bw() +\n  theme(text = element_text(size = 15)) +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 20)) + \n    scale_color_manual(values = color_palette)\n\n\nggplotly(g1, tooltip = c(\"text\"))\n\n\n\n\nCode#TODO: \n# - Improve the 'anon' super/subscript on the bars (or do something else that allows us to quickly see which ones were anonymous)\n# - Sort by average evaluation score\n\n\n\nBelow, we are building an interactive dashboard.6\nShiny dashboard\n\n\n \n\n# Todo: \n- link the code used to generate the shiny app (or something allowing people to see how the data/statistics part iss constructed) \n- 'merits journal' and 'predicted journal' need rescaling in the visualisation (or as a workaround, make them a separate Shiny)\n- \n\n\nYou can see this dashboard on it’s own hosted here.\n\n\n\n\n\n\n\nFuture vis\n\n\n\n\n\nSpider or radial chart\nEach rating is a dimension or attribute (potentially normalized) potentially superimpose a ‘circle’ for the suggested weighting or overall.\nEach paper gets its own spider, with all others (or the average) in faded color behind it as a comparator.\nIdeally user can switch on/off\nBeware – people infer things from the shape’s size\n\n\n\n\n\nCode# JB: what is the purpose of this table? It's very large and I'm not totally\n# sure what it's doing so I'm just turning it off for now\nunit.scale = function(x) (x*100 - min(x*100)) / (max(x*100) - min(x*100))\n\nevaluations_table &lt;- evals_pub %&gt;%\n  select(paper_abbrev, eval_name, cat_1, \n         source_main, overall, adv_knowledge,\n         methods, logic_comms, journal_predict) %&gt;%\n  arrange(desc(paper_abbrev))\n\nformattable(\n  evaluations_table,\n  list(\n    #area(col = 5:8) ~ function(x) percent(x / 100, digits = 0),\n    area(col = 5:8) ~ color_tile(\"#FA614B66\",\"#3E7DCC\"),\n    `journal_predict` = proportion_bar(\"#DeF7E9\", unit.scale)\n  )\n)\n\n\n\nSources of variation\nNext, look for systematic variation in the ratings\n\nBy field and topic area of paper\nBy submission/selection route\nBy evaluation manager (or their seniority, or whether they are US/Commonwealth/Other)7\n\n… perhaps building a model of this. We are looking for systematic ‘biases and trends’, loosely speaking, to help us better understand how our evaluation system is working.\n\nRelationship among the ratings (and predictions)\n\n\n\n\n\n\nNext steps (suggested analyses)\n\n\n\n\n\n\nCorrelation matrix\nANOVA\nPCA (Principle components)\nWith other ‘control’ factors?\n\nHow do the specific measures predict the aggregate ones (overall rating, merited publication)\n\nCF ‘our suggested weighting’\n\n\n\n\n\n\nNext chapter (analysis): aggregation of evaluator judgment\n\n\n\n\n\n\nScoping our future coverage\n\n\n\n\n\nWe have funding to evaluate roughly 50-70 papers/projects per year, given our proposed incentives.\nConsider:\n\nHow many relevant NBER papers come out per year?\nHow much relevant work in other prestige archives?\nWhat quotas do we want (by cause, etc.) and how feasible are these?"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#footnotes",
    "href": "chapters/evaluation_data_analysis.html#footnotes",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "",
    "text": "Consider: timing might be its own section or chapter; this is a major thing journals track, and we want to keep track of ourselves↩︎\nThose marked as ‘considering’ in the Airtable↩︎\nMake interactive/dashboards of the elements below↩︎\nWe use “ub imp” (and “lb imp”) to denote the upper and lower bounds given by evaluators.↩︎\nMore or less, the ones who report a level for ‘conf overall’, although some people did this for some but not others↩︎\nWe are working to enable a range of presentations, aggregations, and analyses (your suggestions are welcome), including reasonable approaches to incorporating evaluator uncertainty↩︎\nDR: My theory is that people in commonwealth countries target a 70+ as ‘strong’ (because of their marking system) and that may drive a bias.↩︎"
  },
  {
    "objectID": "chapters/aggregation.html#notes-on-sources-and-approaches",
    "href": "chapters/aggregation.html#notes-on-sources-and-approaches",
    "title": "\n3  Aggregation of evaluators judgments (modeling)\n",
    "section": "\n3.1 Notes on sources and approaches",
    "text": "3.1 Notes on sources and approaches\n\n\n\n\n\n\nHanea et al\n\n\n\n\n\n(Consult, e.g., repliCATS/Hanea and others work; meta-science and meta-analysis approaches)\naggreCAT package\n\nAlthough the accuracy, calibration, and informativeness of the majority of methods are very similar, a couple of the aggregation methods consistently distinguish themselves as among the best or worst. Moreover, the majority of methods outperform the usual benchmarks provided by the simple average or the median of estimates.\n\nHanea et al, 2021\nHowever, these are in a different context. Most of those measures are designed to deal with probablistic forecasts for binary outcomes, where the predictor also gives a ‘lower bound’ and ‘upper bound’ for that probability. We could roughly compare that to our continuous metrics with 90% CI’s (or imputations for these).\nFurthermore, many (all their successful measures?) use ‘performance-based weights’, accessing metrics from prior prediction performance of the same forecasters We do not have these, nor do we have a sensible proxy for this.\n\n\n\n\n\n\n\n\n\nD Veen et al (2017)\n\n\n\n\n\nlink\n… we show how experts can be ranked based on their knowledge and their level of (un)certainty. By letting experts specify their knowledge in the form of a probability distribution, we can assess how accurately they can predict new data, and how appropriate their level of (un)certainty is. The expert’s specified probability distribution can be seen as a prior in a Bayesian statistical setting. We evaluate these priors by extending an existing prior-data (dis)agreement measure, the Data Agreement Criterion, and compare this approach to using Bayes factors to assess prior specification. We compare experts with each other and the data to evaluate their appropriateness. Using this method, new research questions can be asked and answered, for instance: Which expert predicts the new data best? Is there agreement between my experts and the data? Which experts’ representation is more valid or useful? Can we reach convergence between expert judgement and data? We provided an empirical example ranking (regional) directors of a large financial institution based on their predictions of turnover.\nBe sure to consult the correction made here\n\n\n\n\n\n\n\n\n\nAlso seems relevant:\n\n\n\n\n\nSee Gsheet HERE, generated from an Elicit.org inquiry.\n\n\n\nIn spite of the caveats in the fold above, we construct some measures of aggregate beliefs using the aggrecat package. We will make (and explain) some ad-hoc choices here. We present these:\n\nFor each paper\nFor categories of papers and cross-paper categories of evaluations\nFor the overall set of papers and evaluations\n\nWe can also hold onto these aggregated metrics for later use in modeling.\n\nSimple averaging\nBayesian approaches\nBest-performing approaches from elsewhere\nAssumptions over unit-level random terms\n\nSimple rating aggregation\nExplicit modeling of ‘research quality’ (for use in prizes, etc.)\n\nUse the above aggregation as the outcome of interest, or weight towards categories of greater interest?\nModel with controls – look for greatest positive residual?"
  },
  {
    "objectID": "chapters/aggregation.html#inter-rater-reliability",
    "href": "chapters/aggregation.html#inter-rater-reliability",
    "title": "\n3  Aggregation of evaluators judgments (modeling)\n",
    "section": "\n3.2 Inter-rater reliability",
    "text": "3.2 Inter-rater reliability\nInter-rater reliability is a measure of the degree to which two or more independent raters (in our case, paper evaluators) agree. Here, the ratings are the 7 aspects of each paper that evaluators were asked to rate. For each paper, we can obtain one value that summarizes the agreement between the two or three evaluators. Values closer to 1 indicate evaluators seem to agree on what score to attribute to a given paper across categories, while values close to zero indicate raters do not agree, and negative values indicate that raters have opposing opinions.\n\n\n\n\n\n\nExpand to learn more about why we used Krippendorf’s alpha, and how to interpret it\n\n\n\n\n\nWe use Krippendorff’s alpha as a measure of interrater agreement. Krippendorff’s alpha is a more flexible measure of agreement and can be used with different levels of data (categorical, ordinal, interval, and ratio) as well as different numbers of raters. It automatically accounts for small samples, and allows its coefficient to be compared across sample sizes.\nThe calculation displayed below was done using the function kripp.alpha implemented by Jim Lemon in the package irr and is based on Krippendorff, K. (1980). Content analysis: An introduction to its methodology. Beverly Hills, CA: Sage.\n\nKrippendorff’s alpha can range from -1 to +1, and it can be interpreted similarly to a correlation: values closer to +1 indicate excellent agreement between evaluators; values closer to 0 indicate there is no agreement between evaluators; and negative values indicate that there is systematic disagreement between evaluators beyond what can be expected by chance alone, such that ratings are reversed – where a given evaluator tends to rate something as high, the other(s) tend to rate it as low, and vice versa. Source: Inter-Annotator Agreement: An Introduction to Krippendorff’s Alpha by Andrew Mauboussin\n\nDespite the complexity of the calculations, Krippendorff’s alpha is fundamentally a Kappa-like metric. Its values range from -1 to 1, with 1 representing unanimous agreement between the raters, 0 indicating they’re guessing randomly, and negative values suggesting the raters are systematically disagreeing. (This can happen when raters value different things — for example, if rater A thinks a crowded store is a sign of success, but rater B thinks it proves understaffing and poor management).\nMore information about Krippendorff’s alpha and links to further reading can be found here."
  },
  {
    "objectID": "chapters/aggregation.html#decomposing-variation-dimension-reduction-simple-linear-models",
    "href": "chapters/aggregation.html#decomposing-variation-dimension-reduction-simple-linear-models",
    "title": "\n3  Aggregation of evaluators judgments (modeling)\n",
    "section": "\n3.3 Decomposing variation, dimension reduction, simple linear models",
    "text": "3.3 Decomposing variation, dimension reduction, simple linear models"
  },
  {
    "objectID": "chapters/aggregation.html#later-possiblities",
    "href": "chapters/aggregation.html#later-possiblities",
    "title": "\n3  Aggregation of evaluators judgments (modeling)\n",
    "section": "\n3.4 Later possiblities",
    "text": "3.4 Later possiblities\n\nRelation to evaluation text content (NLP?)\nRelation/prediction of later outcomes (traditional publication, citations, replication)"
  }
]