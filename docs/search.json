[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "",
    "text": "Preface\nThe Unjournal coordinates the public evaluation of hosted papers and dynamically-presented research projects. We are working independently of traditional academic journals to build an open platform and a sustainable system for feedback, ratings, and assessment. Our initial focus is quantitative work that informs global priorities, especially in economics, policy, and other social sciences. We will encourage better research by making it easier for researchers to get feedback and credible ratings on their work. Our aim: to make rigorous research more impactful, and impactful research more rigorous.\nOur main web site, unjournal.org, explains and presents our vision, procedures, and our progress. The ‘output’ evaluations and author (including feedback and discussion) can be found on our PubPub page, and are indexed in scholarly archives.\nThe present site presents data and analysis on The Unjournal’s pipeline and evaluation output\nIn large part:\nWe may expand this analysis further in the future, e.g., to include\nThis resource aims to be:\n10 Aug 2023: David Reinstein and Julia Bottesini have done most of the analysis here."
  },
  {
    "objectID": "chapters/evaluation_data_input.html",
    "href": "chapters/evaluation_data_input.html",
    "title": "\n1  Evaluation data: input/features\n",
    "section": "",
    "text": "Note on data input (10-Aug-23)\n\n\n\n\n\nBelow, the evaluation data is input from an Airtable, which itself was largely hand-input from evaluators’ reports. As PubPub builds (target: end of Sept. 2023), this will allow us to include the ratings and predictions as structured data objects. We then plan to access and input this data directly from the PubPub (API?) into the present analysis. This will improve automation and limit the potential for data entry errors.\n\n\n\n\ninput from airtablebase_id <- \"appbPYEw9nURln7Qg\"\n\n\n# Set your Airtable API key \nSys.setenv(AIRTABLE_API_KEY = Sys.getenv(\"AIRTABLE_API_KEY\"))\n#this should be set in my .Renviron file\n\n# Read data from a specific view\n\nevals <- air_get(base = base_id, \"output_eval\") \n\nall_pub_records <- data.frame()\npub_records <- air_select(base = base_id, table = \"crucial_research\")\n\n# Append the records to the list\nall_pub_records <- bind_rows(all_pub_records, pub_records)\n\n# While the length of the records list is 100 (the maximum), fetch more records\nwhile(nrow(pub_records) == 100) {\n  # Get the ID of the last record in the list\n  offset <- get_offset(pub_records)\n  \n  # Fetch the next 100 records, starting after the last ID\n  pub_records <- air_select(base = base_id, table = \"crucial_research\", offset =  offset)\n  \n  # Append the records to the df\n  all_pub_records <- bind_rows(all_pub_records, pub_records)\n}\n\n\n\njust the useful and publish-able data, clean a bit# clean evals names to snakecase\ncolnames(evals) <- snakecase::to_snake_case(colnames(evals))\n\nevals_pub <- evals %>% \n  dplyr::rename(stage_of_process = stage_of_process_todo_from_crucial_research_2) %>% \n  mutate(stage_of_process = unlist(stage_of_process)) %>% \n  dplyr::filter(stage_of_process == \"published\") %>% \n    select(id, \n           crucial_research, \n           paper_abbrev, \n           evaluator_name, \n           category, \n           source_main, \n           author_agreement, \n           overall, \n           lb_overall, \n           ub_overall, \n           conf_index_overall, \n           advancing_knowledge_and_practice, \n           lb_advancing_knowledge_and_practice, \n           ub_advancing_knowledge_and_practice, \n           conf_index_advancing_knowledge_and_practice,\n           methods_justification_reasonableness_validity_robustness,\n           lb_methods_justification_reasonableness_validity_robustness,\n           ub_methods_justification_reasonableness_validity_robustness,\n           conf_index_methods_justification_reasonableness_validity_robustness, \n           logic_communication, lb_logic_communication, ub_logic_communication, \n           conf_index_logic_communication,\n           engaging_with_real_world_impact_quantification_practice_realism_and_relevance,\n           lb_engaging_with_real_world_impact_quantification_practice_realism_and_relevance,\n           ub_engaging_with_real_world_impact_quantification_practice_realism_and_relevance,\n           conf_index_engaging_with_real_world_impact_quantification_practice_realism_and_relevance,\n           relevance_to_global_priorities, \n           lb_relevance_to_global_priorities, \n           ub_relevance_to_global_priorities, \n           conf_index_relevance_to_global_priorities, \n           journal_quality_predict, \n           lb_journal_quality_predict, \n           ub_journal_quality_predict,\n           conf_index_journal_quality_predict, \n           open_collaborative_replicable, \n           conf_index_open_collaborative_replicable, \n           lb_open_collaborative_replicable, \n           ub_open_collaborative_replicable, \n           merits_journal, \n           lb_merits_journal, \n           ub_merits_journal, \n           conf_index_merits_journal)\n\n# shorten names (before you expand into columns)\nnew_names <- c(\n  \"eval_name\" = \"evaluator_name\",\n  \"cat\" = \"category\",\n  \"crucial_rsx\" = \"crucial_research\",\n  \"conf_overall\" = \"conf_index_overall\",\n  \"adv_knowledge\" = \"advancing_knowledge_and_practice\",\n  \"lb_adv_knowledge\" = \"lb_advancing_knowledge_and_practice\",\n  \"ub_adv_knowledge\" = \"ub_advancing_knowledge_and_practice\",\n  \"conf_adv_knowledge\" = \"conf_index_advancing_knowledge_and_practice\",\n  \"methods\" = \"methods_justification_reasonableness_validity_robustness\",\n  \"lb_methods\" = \"lb_methods_justification_reasonableness_validity_robustness\",\n  \"ub_methods\" = \"ub_methods_justification_reasonableness_validity_robustness\",\n  \"conf_methods\" = \"conf_index_methods_justification_reasonableness_validity_robustness\",\n  \"logic_comms\" = \"logic_communication\",\n  \"lb_logic_comms\" = \"lb_logic_communication\",\n  \"ub_logic_comms\" = \"ub_logic_communication\",\n  \"conf_logic_comms\" = \"conf_index_logic_communication\",\n  \"real_world\" = \"engaging_with_real_world_impact_quantification_practice_realism_and_relevance\",\n  \"lb_real_world\" = \"lb_engaging_with_real_world_impact_quantification_practice_realism_and_relevance\",\n  \"ub_real_world\" = \"ub_engaging_with_real_world_impact_quantification_practice_realism_and_relevance\",\n  \"conf_real_world\" = \"conf_index_engaging_with_real_world_impact_quantification_practice_realism_and_relevance\",\n  \"gp_relevance\" = \"relevance_to_global_priorities\",\n  \"lb_gp_relevance\" = \"lb_relevance_to_global_priorities\",\n  \"ub_gp_relevance\" = \"ub_relevance_to_global_priorities\",\n  \"conf_gp_relevance\" = \"conf_index_relevance_to_global_priorities\",\n  \"journal_predict\" = \"journal_quality_predict\",\n  \"lb_journal_predict\" = \"lb_journal_quality_predict\",\n  \"ub_journal_predict\" = \"ub_journal_quality_predict\",\n  \"conf_journal_predict\" = \"conf_index_journal_quality_predict\",\n  \"open_sci\" = \"open_collaborative_replicable\",\n  \"conf_open_sci\" = \"conf_index_open_collaborative_replicable\",\n  \"lb_open_sci\" = \"lb_open_collaborative_replicable\",\n  \"ub_open_sci\" = \"ub_open_collaborative_replicable\",\n  \"conf_merits_journal\" = \"conf_index_merits_journal\"\n)\n\nevals_pub <- evals_pub %>%\n  rename(!!!new_names)\n\n#  Create a list of labels with the old, longer names\nlabels <- str_replace_all(new_names, \"_\", \" \") %>% str_to_title()\n\n# Assign labels to the dataframe / tibble\n# (maybe this can be done as an attribute, not currently working)\n# for(i in seq_along(labels)) {\n#    col_name <- new_names[names(new_names)[i]]\n#    label <- labels[i]\n#    attr(evals_pub[[col_name]], \"label\") <- label\n#  }\n\n\n# expand categories into columns, unlist everything\nevals_pub %<>%\n  tidyr::unnest_wider(cat, names_sep = \"_\") %>% # give each of these its own col\n  mutate(across(everything(), unlist))  # maybe check why some of these are lists in the first place\n  \n\n# clean the Anonymous names\nevals_pub$eval_name <- ifelse(\n  grepl(\"^\\\\b\\\\w+\\\\b$|\\\\bAnonymous\\\\b\", evals_pub$eval_name),\n  paste0(\"Anonymous_\", seq_along(evals_pub$eval_name)),\n  evals_pub$eval_name\n)\n\n\n#Todo -- check the unlist is not propagating the entry\n#Note: category,  topic_subfield, and source have multiple meaningful categories. These will need care  \n\n\n\n\nReconcile uncertainty ratings and CIs\nWhere people gave only confidence level ‘dots’, we impute CIs (confidence/credible intervals). We follow the correspondence described here. (Otherwise, where they gave actual CIs, we use these.)1\n\n\n\n\n\n\nDots to interval choices\n\n\n\n\n\n\n5 = Extremely confident, i.e., 90% confidence interval spans +/- 4 points or less)\n\nFor 0-100 ratings, code the LB as \\(min(R - 4\\times \\frac{R}{100},0)\\) and the UB as \\(max(R + 4\\times \\frac{R}{100},0)\\), where R is the stated (middle) rating. This ‘scales’ the CI, as interpreted, to be proportional to the rating, with a maximum ‘interval’ of about 8, with the rating is about 96.\n\n4 = Very*confident: 90% confidence interval +/- 8 points or less\n\nFor 0-100 ratings, code the LB as \\(min(R - 8\\times \\frac{R}{100},0)\\) and the UB as \\(max(R + 8\\times \\frac{R}{100},0)\\), where R is the stated (middle) rating.\n\n3 = Somewhat** confident: 90% confidence interval +/- 15 points or less \n\n\n2 = Not very** confident: 90% confidence interval, +/- 25 points or less\n\nComparable scaling for the 2-3 ratings as for the 4 and 5 rating.\n\n1 = Not** confident: (90% confidence interval +/- more than 25 points)\n\nCode LB as \\(min(R - 37.5\\times \\frac{R}{100},0)\\) and the UB as \\(max(R + 37.5\\times \\frac{R}{100},0)\\).\nThis is just a first-pass. There might be a more information-theoretic way of doing this. On the other hand, we might be switching the evaluations to use a different tool soon, perhaps getting rid of the 1-5 confidence ratings altogether.\n\n\n\n\nreconcile explicit bounds and stated confidence level# Define the baseline widths for each confidence rating\nbaseline_widths <- c(4, 8, 15, 25, 37.5)\n\n# Define a function to calculate the lower and upper bounds, where given only an index\ncalc_bounds <- function(rating, confidence, lb_explicit, ub_explicit, scale=100) {\n  # Check if confidence is NA\n  if (is.na(confidence)) {\n    return(c(lb_explicit, ub_explicit))  # Return explicit bounds if confidence is NA\n  } else {\n    baseline_width <- baseline_widths[confidence]\n    lb <- pmax(rating - baseline_width * rating / scale, 0)\n    ub <- pmin(rating + baseline_width * rating / scale, scale)\n    return(c(lb, ub))\n  }\n}\n\n# Function to calculate bounds for a single category\ncalc_category_bounds <- function(df, category, scale=100) {\n  # Calculate bounds\n  bounds <- mapply(calc_bounds, df[[category]], df[[paste0(\"conf_\", category)]], df[[paste0(\"lb_\", category)]], df[[paste0(\"ub_\", category)]])\n  \n  # Convert to data frame and ensure it has the same number of rows as the input\n  bounds_df <- as.data.frame(t(bounds))\n  rownames(bounds_df) <- NULL\n  \n  # Add bounds to original data frame\n  df[[paste0(category, \"_lb_imp\")]] <- bounds_df[, 1]\n  df[[paste0(category, \"_ub_imp\")]] <- bounds_df[, 2]\n  \n  return(df)\n}\n\n\n# Lists of categories\nrating_cats <- c(\"overall\", \"adv_knowledge\", \"methods\", \"logic_comms\", \"real_world\", \"gp_relevance\", \"open_sci\")\n\n#... 'predictions' are currently 1-5 (0-5?)\npred_cats <- c(\"journal_predict\", \"merits_journal\")\n\n#DR: Note that I use these objects in future chapters, but they are not connected to the data frame. Either one saves and reinputs the whole environment (messy, replicability issue), or you save this somewhere and re-input it or connect it to the data frame that gets saved (not sure how to do it), or you hard-code reinput it in the next chapter. \n\n#I do the latter for now, but I'm not happy about it, because the idea is 'input definitions in a single place to use later'\n\n\n# Apply the function to each category\n# DR: I don't love this looping 'edit in place' code approach, but whatever\nfor (cat in rating_cats) {\n  evals_pub <- calc_category_bounds(evals_pub, cat, scale=100)\n}\n\nfor (cat in pred_cats) {\n  evals_pub <- calc_category_bounds(evals_pub, cat, scale=5)\n}\n\n\nWe cannot publicly share the ‘papers under consideration’, but we can share some of the statistics on these papers. Let’s generate an ID (or later, salted hash) for each such paper, and keep only the shareable features of interest\n\nCodeall_papers_p <- all_pub_records %>% \n  dplyr::select(\n    id,\n    category,\n    cfdc_DR,\n     'confidence -- user entered',\n    cfdc_assessor,\n    avg_cfdc,\n    category,\n    cause_cat_1_text,\n    cause_cat_2_text,\n    topic_subfield_text,\n    eval_manager_text,\n    'publication status',\n    'Contacted author?',\n    'stage of process/todo',\n    'source_main',  \n    'author permission?',\n'Direct Kotahi Prize Submission?',\n    'createdTime'         \n  )\n\n\n\nsave data for others’ useall_papers_p %>% saveRDS(file = here(\"data\", \"all_papers_p.Rdata\"))\nall_papers_p %>% write_csv(file = here(\"data\", \"all_papers_p.csv\"))\n\nevals_pub %>% saveRDS(file = here(\"data\", \"evals.Rdata\"))\nevals_pub %>% write_csv(file = here(\"data\", \"evals.csv\"))\n\n#evals_pub %>% readRDS(file = here(\"data\", \"evals.Rdata\"))\n\n\n\n\n\n\n\n\nNote this is only a first-pass; a more sophisticated approach may be warranted in future.↩︎"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html",
    "href": "chapters/evaluation_data_analysis.html",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "",
    "text": "load packageslibrary(tidyverse) \n\n# markdown et al. ----\nlibrary(knitr)\nlibrary(bookdown)\nlibrary(quarto)\nlibrary(formattable) # Create 'Formattable' Data Structures\nlibrary(DT) # R interface to DataTables library (JavaScript)\n\n# dataviz ----\nlibrary(ggrepel)\nlibrary(plotly) # Create Interactive Web Graphics via 'plotly.js'\n\n# others ----\nlibrary(here) # A Simpler Way to Find Your Files\n#devtools::install_github(\"metamelb-repliCATS/aggreCAT\")\n#library(aggrecat)\n\n# Make sure select is always the dplyr version\nselect <- dplyr::select \n\n# options\noptions(knitr.duplicate.label = \"allow\")\noptions(mc.cores = parallel::detectCores())\n\n\n\n\n\n\n\n\nNote on data input (10-Aug-23)\n\n\n\n\n\nBelow, the evaluation data is input from an Airtable, which itself was largely hand-input from evaluators’ reports. As PubPub builds (target: end of Sept. 2023), this will allow us to include the ratings and predictions as structured data objects. We then plan to access and input this data directly from the PubPub (API?) into the present analysis. This will improve automation and limit the potential for data entry errors.\n\n\n\n\nInput evaluation dataevals_pub <- readRDS(file = here(\"data\", \"evals.Rdata\"))\nall_papers_p <- readRDS(file = here(\"data\", \"all_papers_p.Rdata\"))\n\n\n\nDefine lists of columns to use later# Lists of categories\nrating_cats <- c(\"overall\", \"adv_knowledge\", \"methods\", \"logic_comms\", \"real_world\", \"gp_relevance\", \"open_sci\")\n\n#... 'predictions' are currently 1-5 (0-5?)\npred_cats <- c(\"journal_predict\", \"merits_journal\")"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#basic-presentation",
    "href": "chapters/evaluation_data_analysis.html#basic-presentation",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.2 Basic presentation",
    "text": "2.2 Basic presentation\nWhat sorts of papers/projects are we considering and evaluating?\nIn this section, we give some simple data summaries and visualizations, for a broad description of The Unjournal’s coverage.\nIn the interactive table below we give some key attributes of the papers and the evaluators.\n\n\n\nCode(\n  all_evals_dt <- evals_pub %>%\n  arrange(paper_abbrev, eval_name) %>%\n  dplyr::select(paper_abbrev, crucial_rsx, eval_name, cat_1, cat_2, source_main, author_agreement) %>%\n    dplyr::select(-matches(\"ub_|lb_|conf\")) %>% \n    #rename_all(~ gsub(\"_\", \" \", .)) %>% \n    rename(\"Research  _____________________\" = \"crucial_rsx\" \n      ) %>%\n  DT::datatable(\n    caption = \"Evaluations (confidence bounds not shown)\", \n    filter = 'top',\n    rownames= FALSE,\n    options = list(pageLength = 7)\n    )\n)\n\n\n\n\n\n\n\nNext, a preview of the evaluations, focusing on the ‘middle ratings and predictions’:\n\nData datable (all shareable relevant data)(\n  all_evals_dt <- evals_pub %>%\n  arrange(paper_abbrev, eval_name, overall) %>%\n  dplyr::select(paper_abbrev, eval_name, all_of(rating_cats))  %>%\n  DT::datatable(\n    caption = \"Evaluations and predictions (confidence bounds not shown)\", \n    filter = 'top',\n    rownames= FALSE,\n    options = list(pageLength = 7)\n    )\n)\n\n\n\n\n\n\n\n\n\nCode(\n  all_evals_dt_ci <- evals_pub %>%\n  arrange(paper_abbrev, eval_name) %>%\n  dplyr::select(paper_abbrev, eval_name, conf_overall, all_of(rating_cats), matches(\"ub_imp|lb_imp\")) %>%\n  DT::datatable(\n    caption = \"Evaluations and (imputed*) confidence bounds)\", \n    filter = 'top',\n    rownames= FALSE,\n    options = list(pageLength = 7)\n    )\n)\n\n\n\n\n\n\n\n\n\nNext consider…\n\n\n\n\n\n\nComposition of research evaluated\n\nBy field (economics, psychology, etc.)\nBy subfield of economics\nBy topic/cause area (Global health, economic development, impact of technology, global catastrophic risks, etc. )\nBy source (submitted, identified with author permission, direct evaluation)\n\n\nTiming of intake and evaluation1\n\n\n\n\n\nThe funnel plot below starts with the papers we prioritized for likely Unjournal evaluation:2.\n\nCode#Add in the 3 different evaluation input sources\n#update to be automated rather than hard-coded - to look at David's work here\n\npapers_considered <- all_papers_p %>% \n  nrow()\n\npapers_deprio <- all_papers_p %>% \n  filter(`stage of process/todo` ==  \"de-prioritized\") %>% \n  nrow()\n\npapers_evaluated <- all_papers_p %>% \n  filter(`stage of process/todo` %in% c(\"published\",\n                                        \"contacting/awaiting_authors_response_to_evaluation\",\n                                        \"awaiting_publication_ME_comments\",\"awaiting_evaluations\")) %>% \n  nrow()\n\npapers_complete <- all_papers_p %>% \n  filter(`stage of process/todo` ==  \"published\") %>%\n  nrow()\n\npapers_in_progress <-  papers_evaluated - papers_complete\n\npapers_still_in_consideration <-  all_papers_p %>% filter(`stage of process/todo` ==  \"considering\") %>% nrow()\n\n\n#todo: adjust wording of hover notes ('source, target...etc')\n\nfig <- plot_ly(\n    type = \"sankey\",\n    orientation = \"h\",\n\n    node = list(\n      label = c(\"Prioritized\", \"Evaluating\", \"Complete\", \"In progress\", \"Still in consideration\", \"De-prioritized\"),\n      color = c(\"orange\", \"green\", \"green\", \"orange\", \"orange\", \"red\"),\n#Todo: adjust 'location' to group these left to right\n      pad = 15,\n      thickness = 20,\n      line = list(\n        color = \"black\",\n        width = 0.5\n      )\n    ),\n\n    link = list(\n      source = c(0,1,1,0,0),\n      target = c(1,2,3,4,5),\n      value =  c(\n        papers_evaluated,\n        papers_complete,\n        papers_in_progress,\n        papers_still_in_consideration,\n        papers_deprio\n    ))\n  )\nfig <- fig %>% layout(\n    title = \"Unjournal paper funnel\",\n    font = list(\n      size = 10\n    )\n)\n\nfig \n\n\n\n\n\nTodo: 3\n\nCodesummary_df <- evals_pub %>%\n  distinct(crucial_rsx, .keep_all = T) %>% \n  group_by(cat_1) %>%\n  summarise(count = n()) \n\nsummary_df$cat_1[is.na(summary_df$cat_1)] <- \"Unknown\"\n\nsummary_df <- summary_df %>%\n  arrange(-desc(count)) %>%\n  mutate(cat_1 = factor(cat_1, levels = unique(cat_1)))\n\n# Create stacked bar chart\nggplot(summary_df, aes(x = cat_1, y = count)) +\n  geom_bar(stat = \"identity\", color = \"grey30\", fill = \"grey80\") + \n  theme_minimal() +\n  labs(x = \"Paper category\", y = \"Count\", \n       title = \"Count of evaluated papers by primary category\") +\n  theme_bw()\n\n\n\n\n\nCode# Bar plot\nevals_pub %>% \n  rowwise() %>% \n  mutate(source_main = str_replace_all(string = source_main, \n                                       pattern = \"-\", \n                                       replace = \" \") %>% str_to_title()) %>% \n  ggplot(aes(x = source_main)) + \n  geom_bar(position = \"stack\", stat = \"count\", color = \"grey30\", fill = \"grey80\") +\n  labs(x = \"Source\", y = \"Count\") +\n  labs(title = \"Pool of research/evaluations by paper source\") +\n  theme_bw() +\n  theme(text = element_text(size = 15)) +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))\n\n\n\n\n\nCodeall_papers_p$is_evaluated = all_papers_p$`stage of process/todo` %in% c(\"published\",\"contacting/awaiting_authors_response_to_evaluation\",\"awaiting_publication_ME_comments\",\"awaiting_evaluations\") \n\nall_papers_p$source_main[all_papers_p$source_main == \"NA\"] <- \"Not applicable\"  \nall_papers_p$source_main[all_papers_p$source_main == \"internal-from-syllabus-agenda-policy-database\"] <- \"Internal: syllabus, agenda, etc.\"  \nall_papers_p$source_main = tidyr::replace_na(all_papers_p$source_main, \"Unknown\")\n\nall_papers_p %>% \nggplot(aes(x = fct_infreq(source_main), fill = is_evaluated)) + \n  geom_bar(position = \"stack\", stat = \"count\") +\n  labs(x = \"Source\", y = \"Count\", fill = \"Selected for\\nevaluation?\") +\n  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)\n  labs(title = \"Evaluations by source of the paper\") +\n  theme_bw() +\n  theme(text = element_text(size = 15)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))\n\n\n\n\nThe distribution of ratings and predictions\nNext, we present the ratings and predictions along with ‘uncertainty measures’.4 Where evaluators gave only a 1-5 confidence level5, we use the imputations discussed and coded above.\n\nFor each category and prediction (overall and by paper)\n\n\n\nCodewrap_text <- function(text, width) {\n  sapply(strwrap(text, width = width, simplify = FALSE), paste, collapse = \"\\n\")\n}\n\nevals_pub$wrapped_pub_names <- wrap_text(evals_pub$paper_abbrev, width = 15)\n\n\n#todo -- sort by average overall, use color and vertical spacing more\n#todo: introduce a carriage return into the paper names (workaround) to wrap these and save horizontal space\n\n\n# Dot plot\ng1 <- evals_pub %>% \n  ggplot(aes(x = paper_abbrev, y = overall)) +\n  geom_point(aes(color = eval_name), stat = \"identity\", size = 3, shape = 1, stroke = 2) +\n  # geom_text_repel(aes(label = eval_name), \n  #                 size = 3, \n  #                 box.padding = unit(0.35, \"lines\"),\n  #                 point.padding = unit(0.3, \"lines\")) +\n  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)\n  labs(x = \"Paper\", y = \"Overall score\",\n       title = \"Overall scores of evaluated papers\") +\n  theme_bw() +\n  theme(text = element_text(size = 15)) +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))\n#todo -- add more vertical space between papers\n\nggplotly(g1, tooltip = c(\"eval_name\", \"overall\"))\n\n\n\n\n\n\nIn future (todo), we aim to build a dashboard allowing people to use the complete set of ratings and predictions, and choose their own weightings. (Also incorporating the evaluator uncertainty in reasonable ways.)\nThe below should be fixed – the column widths below are misleading\n\n\n\n\n\n\nFuture vis\n\n\n\n\n\nSpider or radial chart\nEach rating is a dimension or attribute (potentially normalized) potentially superimpose a ‘circle’ for the suggested weighting or overall.\nEach paper gets its own spider, with all others (or the average) in faded color behind it as a comparator.\nIdeally user can switch on/off\nBeware – people infer things from the shape’s size\n\n\n\n\n\nCodeunit.scale = function(x) (x*100 - min(x*100)) / (max(x*100) - min(x*100))\nevaluations_table <- evals_pub %>%\n  select(paper_abbrev, eval_name, cat_1, source_main, overall, adv_knowledge, methods, logic_comms, journal_predict) %>%\n  arrange(desc(paper_abbrev))\n\nout = formattable(\n  evaluations_table,\n  list(\n    #area(col = 5:8) ~ function(x) percent(x / 100, digits = 0),\n    area(col = 5:8) ~ color_tile(\"#FA614B66\",\"#3E7DCC\"),\n    `journal_predict` = proportion_bar(\"#DeF7E9\", unit.scale)\n  )\n)\nout\n\n\n\n\n\npaper_abbrev\n\n\neval_name\n\n\ncat_1\n\n\nsource_main\n\n\noverall\n\n\nadv_knowledge\n\n\nmethods\n\n\nlogic_comms\n\n\njournal_predict\n\n\n\n\n\nWell-being: Cash vs. psychotherapy\n\n\nAnonymous_16\n\n\nGH&D\n\n\ninternal-NBER\n\n\n90\n\n\n90\n\n\n90\n\n\n80\n\n\n4.0\n\n\n\n\nWell-being: Cash vs. psychotherapy\n\n\nHannah Metzler\n\n\nGH&D\n\n\ninternal-NBER\n\n\n75\n\n\n70\n\n\n90\n\n\n75\n\n\n3.0\n\n\n\n\nNonprofit Govc.: Randomized healthcare DRC\n\n\nWayne Aaron Sandholtz\n\n\nGH&D\n\n\ninternal-NBER\n\n\n65\n\n\n70\n\n\n60\n\n\n55\n\n\n3.6\n\n\n\n\nLT CEA: Resilient foods vs. AGI safety\n\n\nScott Janzwood\n\n\nlong-term-relevant\n\n\nsubmitted\n\n\n65\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\nLT CEA: Resilient foods vs. AGI safety\n\n\nNA\n\n\nlong-term-relevant\n\n\nsubmitted\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\nLT CEA: Resilient foods vs. AGI safety\n\n\nAnca Hanea\n\n\nlong-term-relevant\n\n\nsubmitted\n\n\n80\n\n\n80\n\n\n70\n\n\n85\n\n\n3.5\n\n\n\n\nLT CEA: Resilient foods vs. AGI safety\n\n\nAlex Bates\n\n\nlong-term-relevant\n\n\nsubmitted\n\n\n40\n\n\n30\n\n\n50\n\n\n60\n\n\n2.0\n\n\n\n\nEnv. fx of prod.: ecological obs\n\n\nElias Cisneros\n\n\nEnvironment\n\n\ninternal-NBER\n\n\n88\n\n\n90\n\n\n75\n\n\n80\n\n\n4.0\n\n\n\n\nEnv. fx of prod.: ecological obs\n\n\nAnonymous_15\n\n\nEnvironment\n\n\ninternal-NBER\n\n\n70\n\n\n70\n\n\n70\n\n\n75\n\n\n4.0\n\n\n\n\nCitizen participation in envt. gov. China\n\n\nRobert Kubinec\n\n\nEnvironment\n\n\nsuggested - externally\n\n\n90\n\n\n90\n\n\n80\n\n\n80\n\n\n4.5\n\n\n\n\nCitizen participation in envt. gov. China\n\n\nAnonymous_6\n\n\nEnvironment\n\n\nsuggested - externally\n\n\n80\n\n\n76\n\n\n72\n\n\n67\n\n\n4.6\n\n\n\n\nCBT Human K, Ghana\n\n\nAnonymous_14\n\n\nGH&D\n\n\ninternal-NBER\n\n\n75\n\n\n60\n\n\n90\n\n\n70\n\n\n4.0\n\n\n\n\nCBT Human K, Ghana\n\n\nAnonymous_19\n\n\nGH&D\n\n\ninternal-NBER\n\n\n75\n\n\n65\n\n\n60\n\n\n75\n\n\nNA\n\n\n\n\nBanning wildlife trade can boost demand\n\n\nAnonymous_4\n\n\nconservation\n\n\nsubmitted\n\n\n75\n\n\n70\n\n\n80\n\n\n70\n\n\n3.0\n\n\n\n\nBanning wildlife trade can boost demand\n\n\nLiew Jia Huan\n\n\nconservation\n\n\nsubmitted\n\n\n75\n\n\n80\n\n\n50\n\n\n70\n\n\n2.5\n\n\n\n\nAdvance market commit. (vaccines)\n\n\nDavid Manheim\n\n\npolicy\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n80\n\n\n25\n\n\n95\n\n\n75\n\n\n3.0\n\n\n\n\nAdvance market commit. (vaccines)\n\n\nJoel Tan\n\n\npolicy\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n79\n\n\n90\n\n\n70\n\n\n70\n\n\n5.0\n\n\n\n\nAdvance market commit. (vaccines)\n\n\nDan Tortorice\n\n\npolicy\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n80\n\n\n90\n\n\n80\n\n\n80\n\n\n4.0\n\n\n\n\nAI and econ. growth\n\n\nSeth Benzell\n\n\nmacroeconomics\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n80\n\n\n75\n\n\n80\n\n\n70\n\n\nNA\n\n\n\n\nAI and econ. growth\n\n\nPhil Trammel\n\n\nmacroeconomics\n\n\ninternal-from-syllabus-agenda-policy-database\n\n\n92\n\n\n97\n\n\n70\n\n\n45\n\n\n3.5\n\n\n\n\n\n\n\nNext, look for systematic variation\n\nBy field and topic area of paper\nBy submission/selection route\nBy evaluation manager\n\n… perhaps building a model of this. We are looking for systematic ‘biases and trends’, loosely speaking, to help us better understand how our evaluation system is working.\n\nRelationship among the ratings (and predictions)\n\nCorrelation matrix\nANOVA\nPCA (Principle components)\nWith other ‘control’ factors?\n\nHow do the specific measures predict the aggregate ones (overall rating, merited publication)\n\nCF ‘our suggested weighting’"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#aggregation-of-expert-opinion-modeling",
    "href": "chapters/evaluation_data_analysis.html#aggregation-of-expert-opinion-modeling",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.3 Aggregation of expert opinion (modeling)",
    "text": "2.3 Aggregation of expert opinion (modeling)"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#notes-on-sources-and-approaches",
    "href": "chapters/evaluation_data_analysis.html#notes-on-sources-and-approaches",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.4 Notes on sources and approaches",
    "text": "2.4 Notes on sources and approaches\n\n\n\n\n\n\nHanea et al\n\n\n\n\n\n(Consult, e.g., repliCATS/Hanea and others work; meta-science and meta-analysis approaches)\naggrecat package\n\nAlthough the accuracy, calibration, and informativeness of the majority of methods are very similar, a couple of the aggregation methods consistently distinguish themselves as among the best or worst. Moreover, the majority of methods outperform the usual benchmarks provided by the simple average or the median of estimates.\n\nHanea et al, 2021\nHowever, these are in a different context. Most of those measures are designed to deal with probablistic forecasts for binary outcomes, where the predictor also gives a ‘lower bound’ and ‘upper bound’ for that probability. We could roughly compare that to our continuous metrics with 90% CI’s (or imputations for these).\nFurthermore, many (all their successful measures?) use ‘performance-based weights’, accessing metrics from prior prediction performance of the same forecasters We do not have these, nor do we have a sensible proxy for this.\n\n\n\n\n\n\n\n\n\nD Veen et al (2017)\n\n\n\n\n\nlink\n… we show how experts can be ranked based on their knowledge and their level of (un)certainty. By letting experts specify their knowledge in the form of a probability distribution, we can assess how accurately they can predict new data, and how appropriate their level of (un)certainty is. The expert’s specified probability distribution can be seen as a prior in a Bayesian statistical setting. We evaluate these priors by extending an existing prior-data (dis)agreement measure, the Data Agreement Criterion, and compare this approach to using Bayes factors to assess prior specification. We compare experts with each other and the data to evaluate their appropriateness. Using this method, new research questions can be asked and answered, for instance: Which expert predicts the new data best? Is there agreement between my experts and the data? Which experts’ representation is more valid or useful? Can we reach convergence between expert judgement and data? We provided an empirical example ranking (regional) directors of a large financial institution based on their predictions of turnover.\nBe sure to consult the correction made here\n\n\n\n\n\n\n\n\n\nAlso seems relevant:\n\n\n\n\n\nSee Gsheet HERE, generated from an Elicit.org inquiry.\n\n\n\nIn spite of the caveats in the fold above, we construct some measures of aggregate beliefs using the aggrecat package. We will make (and explain) some ad-hoc choices here. We present these:\n\nFor each paper\nFor categories of papers and cross-paper categories of evaluations\nFor the overall set of papers and evaluations\n\nWe can also hold onto these aggregated metrics for later use in modeling.\n\nSimple averaging\nBayesian approaches\nBest-performing approaches from elsewhere\nAssumptions over unit-level random terms\n\nExplicit modeling of ‘research quality’ (for use in prizes, etc.)\n\nUse the above aggregation as the outcome of interest, or weight towards categories of greater interest?\nModel with controls – look for greatest positive residual?"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#inter-rater-reliability",
    "href": "chapters/evaluation_data_analysis.html#inter-rater-reliability",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.5 Inter-rater reliability",
    "text": "2.5 Inter-rater reliability"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#decomposing-variation-dimension-reduction-simple-linear-models",
    "href": "chapters/evaluation_data_analysis.html#decomposing-variation-dimension-reduction-simple-linear-models",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.6 Decomposing variation, dimension reduction, simple linear models",
    "text": "2.6 Decomposing variation, dimension reduction, simple linear models"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#later-possiblities",
    "href": "chapters/evaluation_data_analysis.html#later-possiblities",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.7 Later possiblities",
    "text": "2.7 Later possiblities\n\nRelation to evaluation text content (NLP?)\nRelation/prediction of later outcomes (traditional publication, citations, replication)"
  },
  {
    "objectID": "chapters/evaluation_data_analysis.html#scoping-our-future-coverage",
    "href": "chapters/evaluation_data_analysis.html#scoping-our-future-coverage",
    "title": "\n2  Evaluation data: description, exploration, checks\n",
    "section": "\n2.8 Scoping our future coverage",
    "text": "2.8 Scoping our future coverage\nWe have funding to evaluate roughly 50-70 papers/projects per year, given our proposed incentives.\nConsider:\n\nHow many relevant NBER papers come out per year?\nHow much relevant work in other prestige archives?\nWhat quotas do we want (by cause, etc.) and how feasible are these?"
  },
  {
    "objectID": "index.html#collophon",
    "href": "index.html#collophon",
    "title": "The Unjournal evaluations: data and analysis",
    "section": "Collophon",
    "text": "Collophon\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\nCodeoptions(knitr.duplicate.label = \"allow\")"
  }
]