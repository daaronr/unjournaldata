{
  "hash": "0fc5708385141dc10e9b51ed160f7a97",
  "result": {
    "markdown": "# Evaluation data: description, exploration, checks\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"load packages\"}\n#devtools::install_github(\"rethinkpriorities/rp-r-package\")\nlibrary(rethinkpriorities)\n\n#devtools::install_github(\"rethinkpriorities/r-noodling-package\")\nlibrary(rnoodling)\n\nlibrary(here)\nsource(here::here(\"code\", \"shared_packages_code.R\"))\nlibrary(dplyr)\nlibrary(pacman)\n\np_load(santoku, lme4, huxtable, janitor, emmeans, sjPlot, sjmisc, ggeffects, ggrepel, likert, labelled, plotly, stringr, install=FALSE)\n\np_load(ggthemes, paletteer, ggridges, install=FALSE)\n\nselect <- dplyr::select\n\noptions(knitr.duplicate.label = \"allow\")\n\noptions(mc.cores = parallel::detectCores())\n#rstan_options(auto_write = TRUE)\n\n#library(hunspell)\n\n#(brms)\n\n#devtools::install_github(\"bergant/airtabler\")\np_load(airtabler)\n\n#remotes::install_github(\"rmcelreath/rethinking\")\n#library(rethinking)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"input from airtable\"}\nbase_id <- \"appbPYEw9nURln7Qg\"\n\n# Set your Airtable API key\n#Sys.setenv(AIRTABLE_API_KEY = \"\") \n#this should be set in my .Renviron file\n\n\n# Read data from a specific view\nevals <- air_get(base = \"appbPYEw9nURln7Qg\", \"output_eval\") \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"just the useful and publish-able data, clean a bit\"}\ncolnames(evals) <- snakecase::to_snake_case(colnames(evals))\n\nevals_pub <- evals %>% \n  dplyr::rename(stage_of_process = stage_of_process_todo_from_crucial_research_2) %>% \n  mutate(stage_of_process = unlist(stage_of_process)) %>% \n  dplyr::filter(stage_of_process == \"published\") %>% \n    select(id, evaluator_name, crucial_research, overall, lb_overall, ub_overall, conf_index_overall, advancing_knowledge_and_practice, lb_advancing_knowledge_and_practice, ub_advancing_knowledge_and_practice, conf_index_advancing_knowledge_and_practice, methods_justification_reasonableness_validity_robustness, lb_methods_justification_reasonableness_validity_robustness, ub_methods_justification_reasonableness_validity_robustness, conf_index_methods_justification_reasonableness_validity_robustness, logic_communication, lb_logic_communication, ub_logic_communication, conf_index_logic_communication, engaging_with_real_world_impact_quantification_practice_realism_and_relevance, lb_engaging_with_real_world_impact_quantification_practice_realism_and_relevance, ub_engaging_with_real_world_impact_quantification_practice_realism_and_relevance, conf_index_engaging_with_real_world_impact_quantification_practice_realism_and_relevance, relevance_to_global_priorities, lb_relevance_to_global_priorities, ub_relevance_to_global_priorities, conf_index_relevance_to_global_priorities, journal_quality_predict, lb_journal_quality_predict, ub_journal_quality_predict, conf_index_journal_quality_predict, open_collaborative_replicable, conf_index_open_collaborative_replicable, lb_open_collaborative_replicable, ub_open_collaborative_replicable, merits_journal, lb_merits_journal, ub_merits_journal, conf_index_merits_journal)\n\nevals_pub %<>%\n   mutate(across(everything(), unlist)) #unlist list columns\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Shorten names \"}\nnew_names <- c(\n  \"eval_name\" = \"evaluator_name\",\n  \"crucial_rsx\" = \"crucial_research\",\n  \"conf_overall\" = \"conf_index_overall\",\n  \"adv_knowledge\" = \"advancing_knowledge_and_practice\",\n  \"lb_adv_knowledge\" = \"lb_advancing_knowledge_and_practice\",\n  \"ub_adv_knowledge\" = \"ub_advancing_knowledge_and_practice\",\n  \"conf_adv_knowledge\" = \"conf_index_advancing_knowledge_and_practice\",\n  \"methods\" = \"methods_justification_reasonableness_validity_robustness\",\n  \"lb_methods\" = \"lb_methods_justification_reasonableness_validity_robustness\",\n  \"ub_methods\" = \"ub_methods_justification_reasonableness_validity_robustness\",\n  \"conf_methods\" = \"conf_index_methods_justification_reasonableness_validity_robustness\",\n  \"logic_comms\" = \"logic_communication\",\n  \"lb_logic_comms\" = \"lb_logic_communication\",\n  \"ub_logic_comms\" = \"ub_logic_communication\",\n  \"conf_logic_comms\" = \"conf_index_logic_communication\",\n  \"real_world\" = \"engaging_with_real_world_impact_quantification_practice_realism_and_relevance\",\n  \"lb_real_world\" = \"lb_engaging_with_real_world_impact_quantification_practice_realism_and_relevance\",\n  \"ub_real_world\" = \"ub_engaging_with_real_world_impact_quantification_practice_realism_and_relevance\",\n  \"conf_real_world\" = \"conf_index_engaging_with_real_world_impact_quantification_practice_realism_and_relevance\",\n  \"gp_relevance\" = \"relevance_to_global_priorities\",\n  \"lb_gp_relevance\" = \"lb_relevance_to_global_priorities\",\n  \"ub_gp_relevance\" = \"ub_relevance_to_global_priorities\",\n  \"conf_gp_relevance\" = \"conf_index_relevance_to_global_priorities\",\n  \"journal_predict\" = \"journal_quality_predict\",\n  \"lb_journal_predict\" = \"lb_journal_quality_predict\",\n  \"ub_journal_predict\" = \"ub_journal_quality_predict\",\n  \"conf_journal_predict\" = \"conf_index_journal_quality_predict\",\n  \"open_sci\" = \"open_collaborative_replicable\",\n  \"conf_open_sci\" = \"conf_index_open_collaborative_replicable\",\n  \"lb_open_sci\" = \"lb_open_collaborative_replicable\",\n  \"ub_open_sci\" = \"ub_open_collaborative_replicable\",\n  \"conf_merits_journal\" = \"conf_index_merits_journal\"\n)\n\n\nevals_pub <- evals_pub %>%\n  rename(!!!new_names)\n\n# make the old names into labels\n\nlibrary(stringr)\n \n#  Create a list of labels\nlabels <- str_replace_all(new_names, \"_\", \" \")\nlabels <- str_to_title(labels)\n \n# Assign labels to the dataframe\n# for(i in seq_along(labels)) {\n#    col_name <- new_names[names(new_names)[i]]\n#    label <- labels[i]\n#    attr(evals_pub[[col_name]], \"label\") <- label\n#  }\n# \n```\n:::\n\n\n<!-- need airtable API stuff -->\n\n### Reconcile the uncertainty ratings and CIs (first-pass) {-}\n\nImpute CIs from stated confidence level 'dots', correspondence loosely described [here](https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#1-5-dots-explanation-and-relation-to-cis)\n\n::: {.callout-note collapse=\"true\"}\n## Dots to interval choices\n\n> 5 = Extremely confident, i.e., 90% confidence interval spans +/- 4 points or less)\n\nFor 0-100 ratings, code the LB as $min(R - 4\\times \\frac{R}{100},0)$ and the UB as $max(R + 4\\times \\frac{R}{100},0)$, where R is the stated (middle) rating. This 'scales' the CI, as interpreted, to be proportional to the rating, with a  maximum 'interval' of about 8, with the rating is about 96.\n\n> 4 = Very*confident: 90% confidence interval +/- 8 points or less\n\nFor 0-100 ratings, code the LB as $min(R - 8\\times \\frac{R}{100},0)$ and the UB as $max(R + 8\\times \\frac{R}{100},0)$, where R is the stated (middle) rating. \n\n> 3 = Somewhat** confident: 90% confidence interval +/- 15 points or less&#x20;\n\n> 2 = Not very** confident: 90% confidence interval, +/- 25 points or less\n\nComparable scaling for the 2-3 ratings as for the 4 and 5 rating.\n\n> 1 = Not** confident: (90% confidence interval +/- more than 25 points)\n    \nCode LB as $min(R - 37.5\\times \\frac{R}{100},0)$ and the UB as $max(R + 37.5\\times \\frac{R}{100},0)$. \n    \nThis is just a first-pass. There might be a more information-theoretic way of doing this. On the other hand, we might be switching the evaluations to use a different tool soon, perhaps getting rid of the 1-5 confidence ratings.\n\n::: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"reconcile explicit bounds and stated confidence level\"}\n# Define the baseline widths for each confidence rating\nbaseline_widths <- c(4, 8, 15, 25, 37.5)\n\n# Define a function to calculate the lower and upper bounds, where given only an index\ncalc_bounds <- function(rating, confidence, lb_explicit, ub_explicit, scale=100) {\n  # Check if confidence is NA\n  if (is.na(confidence)) {\n    return(c(lb_explicit, ub_explicit))  # Return explicit bounds if confidence is NA\n  } else {\n    baseline_width <- baseline_widths[confidence]\n    lb <- pmax(rating - baseline_width * rating / scale, 0)\n    ub <- pmin(rating + baseline_width * rating / scale, scale)\n    return(c(lb, ub))\n  }\n}\n\n# Function to calculate bounds for a single category\ncalc_category_bounds <- function(df, category, scale=100) {\n  # Calculate bounds\n  bounds <- mapply(calc_bounds, df[[category]], df[[paste0(\"conf_\", category)]], df[[paste0(\"lb_\", category)]], df[[paste0(\"ub_\", category)]])\n  \n  # Convert to data frame and ensure it has the same number of rows as the input\n  bounds_df <- as.data.frame(t(bounds))\n  rownames(bounds_df) <- NULL\n  \n  # Add bounds to original data frame\n  df[[paste0(category, \"_lb_imp\")]] <- bounds_df[, 1]\n  df[[paste0(category, \"_ub_imp\")]] <- bounds_df[, 2]\n  \n  return(df)\n}\n\n\n# Lists of categories\n\nrating_cats <- c(\"overall\", \"adv_knowledge\", \"methods\", \"logic_comms\", \"real_world\", \"gp_relevance\", \"open_sci\")\n\n#... 'predictions' are currently 1-5 (0-5?)\npred_cats <- c(\"journal_predict\", \"merits_journal\")\n\n# Apply the function to each category\n# DR: I don't love this looping 'edit in place' code approach, but whatever\nfor (cat in rating_cats) {\n  evals_pub <- calc_category_bounds(evals_pub, cat, scale=100)\n}\n\nfor (cat in pred_cats) {\n  evals_pub <- calc_category_bounds(evals_pub, cat, scale=5)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"save data for others' use\"}\nevals_pub %>% saveRDS(file = here(\"data\", \"evals.Rdata\"))\nevals_pub %>% write_csv(file = here(\"data\", \"evals.csv\"))\n```\n:::\n\n\n    \n# Basic presentation\n\n- Simple data summaries/codebooks/dashboards and visualization\n\n\n### The distribution of ratings and predictions {-}\n\n- For each category and prediction (overall and by paper)\n\n- By field and topic area of paper\n\n- By submission/selection route\n\n- By evaluation manager\n\n\n### Relationship among the ratings (and predictions) {-} \n\n- Correlation matrix\n\n- ANOVA\n\n- PCI\n\n- With other 'control' factors?\n\n- How do the specific measures predict the aggregate ones (overall rating, merited publication)\n    - CF 'our suggested weighting'\n\n\n## Aggregation of expert opinion (modeling)\n\n(Consult, e.g., repliCATS/Hanea and others work; meta-science and meta-analysis approaches)\n\n- Simple averaging\n\n- Bayesian approaches \n\n- Assumptions over unit-level random terms \n\n### Explicit modeling of 'research quality' (for use in prizes, etc.) {-}\n\n- Use the above aggregation as the outcome of interest, or weight towards categories of greater interest?\n\n- Model with controls -- look for greatest positive residual?   \n\n\n## Inter-rater reliability\n\n## Decomposing variation, dimension reduction, simple linear models\n\n\n## Later possiblities\n\n- Relation to evaluation text content (NLP?)\n\n- Relation/prediction of later outcomes (traditional publication, citations, replication)\n\n\n",
    "supporting": [
      "evaluation_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}