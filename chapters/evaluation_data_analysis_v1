# Evaluation data: description, exploration, checks

## Data input, cleaning, feature construction and imputation

```{r setup, warning=FALSE}
#| label: load-packages
#| code-summary: "load packages"

library(tidyverse)

# markdown et al. ----
library(knitr)
library(bookdown)
library(quarto)
library(formattable) # Create 'Formattable' Data Structures
library(DT) # R interface to DataTables library (JavaScript)

# dataviz ----
library(ggrepel)
library(plotly) # Create Interactive Web Graphics via 'plotly.js'

# others ----
library(here) # A Simpler Way to Find Your Files
# renv::install(packages = "metamelb-repliCATS/aggreCAT")
library(aggreCAT)

# Make sure select is always the dplyr version
select <- dplyr::select

# options
options(knitr.duplicate.label = "allow")
options(mc.cores = parallel::detectCores())

```


::: {.callout-note collapse="true"}
## Note on data input (10-Aug-23)

Below, the evaluation data is input from an Airtable, which itself was largely hand-input from evaluators' reports. As PubPub builds (target: end of Sept. 2023), this will allow us to  include the ratings and predictions as structured data objects. We then plan to access and input this data *directly* from the PubPub (API?) into the present analysis. This will improve automation and limit the potential for data entry errors.

:::

```{r}
#| label: input_eval_data
#| code-summary: "Input evaluation data"

evals_pub <- readRDS(file = here("data", "evals.Rdata"))
all_papers_p <- readRDS(file = here("data", "all_papers_p.Rdata"))

```



```{r}
#| label: list_of_columns
#| code-summary: "Define lists of columns to use later"

# Lists of categories
rating_cats <- c("overall", "adv_knowledge", "methods", "logic_comms", "real_world", "gp_relevance", "open_sci")

#... 'predictions' are currently 1-5 (0-5?)
pred_cats <- c("journal_predict", "merits_journal")

```



## Basic presentation

### What sorts of papers/projects are we considering and evaluating? {-}

In this section, we give some simple data summaries and visualizations, for a broad description of The Unjournal's coverage.

In the interactive table below we give some key attributes of the papers and the evaluators.


::: column-body-outset

<!-- Todo: make this smaller, get it to fit on the page better, more user-friendly -->

#### Data Tables {-}

```{r }
#| label: datatable0
# Need to find a way to control column width but it seems to be a problem with DT
# https://github.com/rstudio/DT/issues/29

# we didn't seem to be using all_evals_dt so I removed it to increase readability


evals_pub_dt <- evals_pub %>%
  # Arrange data
  arrange(paper_abbrev, eval_name, overall) %>%

  # Select and rename columns
  dplyr::select(paper_abbrev, eval_name, all_of(rating_cats)) %>%
  rename(
    "Paper Abbreviation" = paper_abbrev,
    "Evaluator Name" = eval_name,
    "Advancing knowledge" = adv_knowledge,
    "Methods" = methods,
    "Logic and communication" = logic_comms
    # Add more renaming here if needed
  ) %>%

  # Convert to a datatable and apply styling
  datatable(
    caption = "Evaluations and predictions (confidence bounds not shown)",
    filter = 'top',
    rownames = FALSE,
    options = list(pageLength = 10)
  )

columns_to_center <- colnames(evals_pub_dt)[-(1)]

evals_pub_dt %>%
  formatStyle(
    columns = c('Paper Abbreviation', 'Evaluator Name'),
    fontSize = '10px', fontWeight = 'bold'
  ) %>%
  formatStyle(
    columns = columns_to_center,
    textAlign = 'center'
  )




```


\

Next, a preview of the evaluations, focusing on the 'middle ratings and predictions':

```{r }
#| label: datatable
#| code-summary: "Data datable (all shareable relevant data)"

# we didn't seem to be using all_evals_dt so I removed it to increase readability


evals_pub %>%
  arrange(paper_abbrev, eval_name, overall) %>%
  dplyr::select(paper_abbrev, eval_name, all_of(rating_cats))  %>%
  DT::datatable(
    caption = "Evaluations and predictions (confidence bounds not shown)",
    filter = 'top',
    rownames= FALSE,
    options = list(pageLength = 5)
    )

```
\

<!-- Todo -- Present these, including bounds, in a useful way -->

```{r eval=FALSE}

# we did not seem to be using all_evals_dt_ci so I removed it to improve readability
evals_pub %>%
  arrange(paper_abbrev, eval_name) %>%
  dplyr::select(paper_abbrev, eval_name, conf_overall, all_of(rating_cats), matches("ub_imp|lb_imp")) %>%
  DT::datatable(
    caption = "Evaluations and (imputed*) confidence bounds)",
    filter = 'top',
    rownames= FALSE,
    options = list(pageLength = 5)
    )


```
:::





::: {.callout-note collapse="true"}
##### Next consider...

- Composition of research evaluated
     - By field (economics, psychology, etc.)
     - By subfield of economics
     - By topic/cause area (Global health, economic development, impact of technology, global catastrophic risks, etc.    )
     - By source (submitted, identified with author permission, direct evaluation)

- Timing of intake and evaluation^[Consider: timing might be its own section or chapter; this is a major thing journals track, and we want to keep track of ourselves]

:::

#### Paper selection {-}

The Sankey diagram below starts with the papers we prioritized for likely *Unjournal* evaluation:^[Those marked as 'considering' in the Airtable].

```{r sankey plot}

#Add in the 3 different evaluation input sources
#update to be automated rather than hard-coded - to look at David's work here

papers_considered <- all_papers_p %>%
  nrow()

papers_deprio <- all_papers_p %>%
  filter(`stage of process/todo` ==  "de-prioritized") %>%
  nrow()

papers_evaluated <- all_papers_p %>%
  filter(`stage of process/todo` %in% c("published",
                                        "contacting/awaiting_authors_response_to_evaluation",
                                        "awaiting_publication_ME_comments","awaiting_evaluations")) %>%
  nrow()

papers_complete <- all_papers_p %>%
  filter(`stage of process/todo` ==  "published") %>%
  nrow()

papers_in_progress <-  papers_evaluated - papers_complete

papers_still_in_consideration <-  all_papers_p %>% filter(`stage of process/todo` ==  "considering") %>% nrow()


#todo: adjust wording of hover notes ('source, target...etc')

fig <- plot_ly(
  type = "sankey",
  orientation = "h",

  node = list(
    label = c("Prioritized", "Evaluating", "Complete", "In progress", "Still in consideration", "De-prioritized"),
    color = c("orange", "green", "green", "orange", "orange", "red"),
    #Todo: adjust 'location' to group these left to right
    pad = 15,
    thickness = 20,
    line = list(
      color = "black",
      width = 0.5
    )
  ),

  link = list(
    source = c(0,1,1,0,0),
    target = c(1,2,3,4,5),
    value =  c(
      papers_evaluated,
      papers_complete,
      papers_in_progress,
      papers_still_in_consideration,
      papers_deprio
    ))
)
fig <- fig %>% layout(
  title = "Unjournal paper funnel",
  font = list(
    size = 10
  )
)

fig

```


Todo: ^[Make interactive/dashboards of the elements below]

#### Paper categories {-}


```{r all_categories}
evals_pub %>%
  select(paper_abbrev, starts_with("cat_")) %>%
  pivot_longer(cols = starts_with("cat_"), names_to = "CatNum", values_to = "Category") %>%
  group_by(CatNum, Category) %>%
  count() %>%
  filter(!is.na(Category)) %>%
  mutate(Category = str_to_title(Category),
         CatNum = ordered(CatNum,
                          levels = c("cat_1", "cat_2", "cat_3"),
                          labels = c("Primary", "Secondary", "Tertiary"))) %>%
  ggplot(aes(x = reorder(Category, -n), y = n)) +
  geom_bar(aes(fill = CatNum), stat = "identity", color = "grey30") +
  labs(x = "Paper category", y = "Count", fill = "Cat Level",
       title = "Paper categories represented in pilot data") +
  theme_bw() +
  facet_grid(~CatNum, scales="free_x", space="free_x") +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  theme(legend.position = "none")

```

#### Paper source {-}

```{r paper_source}

# Bar plot
evals_pub %>%
  rowwise() %>%
  mutate(source_main = str_replace_all(string = source_main,
                                       pattern = "-",
                                       replace = " ") %>% str_to_title()) %>%
  ggplot(aes(x = source_main)) +
  geom_bar(position = "stack", stat = "count", color = "grey30", fill = "grey80") +
  labs(x = "Source", y = "Count") +
  labs(title = "Pool of research/evaluations by paper source") +
  theme_bw() +
  theme(text = element_text(size = 15)) +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))

```

```{r data_clean_}
# JB: Most of these should probably be cleaned in data storage

# paper statuses that are considered "being evaluated"
eval_true = c("published",
              "contacting/awaiting_authors_response_to_evaluation",
              "awaiting_publication_ME_comments",
              "awaiting_evaluations")

# Is the paper being evaluated?
all_papers_p <- all_papers_p %>%
  mutate(is_evaluated = if_else(`stage of process/todo` %in% eval_true, TRUE, FALSE))

# main source clean
all_papers_p <- all_papers_p %>%
  mutate(source_main = case_when(source_main == "NA" ~ "Not applicable",
                                 source_main == "internal-from-syllabus-agenda-policy-database" ~ "Internal: syllabus, agenda, etc.",
                                 is.na(source_main) ~ "Unknown",
                                 TRUE ~ source_main))

all_papers_p %>%
ggplot(aes(x = fct_infreq(source_main), fill = is_evaluated)) +
  geom_bar(position = "stack", stat = "count") +
  labs(x = "Source", y = "Count", fill = "Selected for\nevaluation?") +
  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)
  labs(title = "Evaluations by source of the paper") +
  theme_bw() +
  theme(text = element_text(size = 15)) +
  scale_fill_brewer(palette = "Set1") +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))
```


## The distribution of ratings and predictions {-}

Next, we present the ratings and predictions along with 'uncertainty measures'.^[We use "ub imp" (and "lb imp") to denote the upper and lower bounds given by evaluators.] Where evaluators gave only a 1-5 confidence level^[More or less, the ones who report a level for 'conf overall', although some people did this for some but not others], we use the imputations discussed and coded above.


- For each category and prediction (overall and by paper)
```{r CI_bounds}

evals_pub %>%
  select(matches("overall")) %>%
  view()

```


::: column-body-outset


```{r}
#| fig-height: 9

#todo -- sort by average overall, use color and vertical spacing more

pd = position_dodge(width = 0.8)

# Dot plot
g1 <- evals_pub %>%
  ggplot(aes(x = paper_abbrev, y = overall,
             text = paste0('Evaluator: ', eval_name,   # tooltip data
                           '<br>Rating [CI]: ', overall, " [", overall_lb_imp, ", ", overall_ub_imp, "]"))) +
  geom_point(aes(color = eval_name),
             stat = "identity", size = 2, shape = 18, stroke = 1,
             position = pd) +
  geom_linerange(aes(ymin = overall_lb_imp, ymax = overall_ub_imp, color = eval_name), position = pd) +
  coord_flip() + # flipping the coordinates to have categories on y-axis (on the left)
  labs(x = "Paper", y = "Overall score",
       title = "Overall scores of evaluated papers") +
  theme_bw() +
  theme(text = element_text(size = 15)) +
  theme(legend.position = "none") +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 20))


ggplotly(g1, tooltip = c("text"))
```
:::


In future (todo), we aim to build a dashboard allowing people to use the complete set of ratings and predictions, and choose their own weightings. (Also incorporating the evaluator uncertainty in reasonable ways.)

*The below should be fixed -- the column widths below are misleading*

::: {.callout-note collapse="true"}
## Future vis

Spider or radial chart

Each rating is a dimension or attribute (potentially normalized)
potentially superimpose a 'circle' for the suggested weighting or overall.

Each paper gets its own spider, with all others (or the average) in faded color behind it as a comparator.

Ideally user can switch on/off

Beware -- people infer things from the shape's size

:::


::: column-body-outset

```{r, eval=FALSE}
# JB: what is the purpose of this table? It's very large and I'm not totally
# sure what it's doing so I'm just turning it off for now
unit.scale = function(x) (x*100 - min(x*100)) / (max(x*100) - min(x*100))

evaluations_table <- evals_pub %>%
  select(paper_abbrev, eval_name, cat_1,
         source_main, overall, adv_knowledge,
         methods, logic_comms, journal_predict) %>%
  arrange(desc(paper_abbrev))

formattable(
  evaluations_table,
  list(
    #area(col = 5:8) ~ function(x) percent(x / 100, digits = 0),
    area(col = 5:8) ~ color_tile("#FA614B66","#3E7DCC"),
    `journal_predict` = proportion_bar("#DeF7E9", unit.scale)
  )
)


```
:::

### Simple rating aggregation

```{r babys_first_ratings_agg, eval=FALSE}

evals_pub %>%
  select(id, eval_name, paper_abbrev,
         overall, overall_lb_imp, overall_ub_imp,
         adv_knowledge, adv_knowledge_lb_imp, adv_knowledge_ub_imp,
         methods, methods_lb_imp, methods_ub_imp,
         logic_comms, logic_comms_lb_imp, logic_comms_ub_imp,
         real_world, real_world_lb_imp, real_world_ub_imp,
         gp_relevance, gp_relevance_lb_imp, gp_relevance_ub_imp,
         open_sci, open_sci_lb_imp, open_sci_ub_imp) %>%
  rename_with(function(x) paste0(x,"_score"), all_of(rating_cats)) %>%
  pivot_longer(cols = c(-id, -eval_name, -paper_abbrev),
              names_pattern = "(.+)_(score|[ul]b_imp)",
              names_to = c("criterion","element"),
              values_to = "value") -> paper_ratings

paper_ratings <- paper_ratings %>%
  rename(paper_id = paper_abbrev,
         user_name = eval_name) %>%
  mutate(round = "round_1",
         element = case_when(element == "lb_imp" ~ "three_point_lower",
                             element == "ub_imp" ~ "three_point_upper",
                             element == "score" ~ "three_point_best"))

paper_ratings %>%
  filter(criterion == "overall") %>%
  group_by(user_name, paper_id) %>%
  filter(sum(is.na(value))==0) %>%
  ungroup() -> temp


AverageWAgg(expert_judgements = temp, round_2_filter = FALSE, type = "ArMean")

IntervalWAgg(expert_judgements = temp, round_2_filter = FALSE, type = "IntWAgg")

aggreCAT::DistributionWAgg(expert_judgements = temp, round_2_filter = FALSE, type = "DistribArMean", percent_toggle = T)

# EXAMPLE CODE ===============================
data(data_ratings)
set.seed(1234)

participant_subset <- data_ratings %>%
  distinct(user_name) %>%
  sample_n(5) %>%
  mutate(participant_name = paste("participant", rep(1:n())))

single_claim <- data_ratings %>%
  filter(paper_id == "28") %>%
  right_join(participant_subset, by = "user_name") %>%
  filter(grepl(x = element, pattern = "three_.+")) %>%
  select(-group, -participant_name, -question)

DistributionWAgg(expert_judgements = single_claim,
            type = "DistribArMean", percent_toggle = T)



```

Next, look for systematic variation

- By field and topic area of paper

- By submission/selection route

- By evaluation manager

... perhaps building a model of this. We are looking for systematic 'biases and trends', loosely speaking, to help us better understand how our evaluation system is working.

\




### Relationship among the ratings (and predictions) {-}

- Correlation matrix

- ANOVA

- PCA (Principle components)

- With other 'control' factors?

- How do the specific measures predict the aggregate ones (overall rating, merited publication)
    - CF 'our suggested weighting'


## Aggregation of expert opinion (modeling)


## Notes on sources and approaches


::: {.callout-note collapse="true"}

## Hanea et al {-}
(Consult, e.g., repliCATS/Hanea and others work; meta-science and meta-analysis approaches)

`aggrecat` package

> Although the accuracy, calibration, and informativeness of the majority of methods are very similar, a couple of the aggregation methods consistently distinguish themselves as among the best or worst. Moreover, the majority of methods outperform the usual benchmarks provided by the simple average or the median of estimates.

[Hanea et al, 2021](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256919#sec007)

 However, these are in a different context. Most of those measures are designed to deal with probablistic forecasts for binary outcomes, where the predictor also gives a 'lower bound' and 'upper bound' for that probability. We could roughly compare that to our continuous metrics with 90% CI's (or imputations for these).

Furthermore, many (all their successful measures?) use 'performance-based weights', accessing metrics from prior prediction performance of the same forecasters We do not have these, nor do we have a sensible proxy for this.
:::


::: {.callout-note collapse="true"}
## D Veen et al (2017)

[link](https://www.researchgate.net/profile/Duco-Veen/publication/319662351_Using_the_Data_Agreement_Criterion_to_Rank_Experts'_Beliefs/links/5b73e2dc299bf14c6da6c663/Using-the-Data-Agreement-Criterion-to-Rank-Experts-Beliefs.pdf)

... we show how experts can be ranked based on their knowledge and their level of (un)certainty. By letting experts specify their knowledge in the form of a probability distribution, we can assess how accurately they can predict new data, and how appropriate their level of (un)certainty is. The expert’s specified probability distribution can be seen as a prior in a Bayesian statistical setting. We evaluate these priors by extending an existing prior-data (dis)agreement measure, the Data Agreement Criterion, and compare this approach to using Bayes factors to assess prior specification. We compare experts with each other and the data to evaluate their appropriateness. Using this method, new research questions can be asked and answered, for instance: Which expert predicts the new data best? Is there agreement between my experts and the data? Which experts’ representation is more valid or useful? Can we reach convergence between expert judgement and data? We provided an empirical example ranking (regional) directors of a large financial institution based on their predictions of turnover.

Be sure to consult the [correction made here](https://www.semanticscholar.org/paper/Correction%3A-Veen%2C-D.%3B-Stoel%2C-D.%3B-Schalken%2C-N.%3B-K.%3B-Veen-Stoel/a2882e0e8606ef876133f25a901771259e7033b1)

:::


::: {.callout-note collapse="true"}
## Also seems relevant:

See [Gsheet HERE](https://docs.google.com/spreadsheets/d/14japw6eLGpGjEWy1MjHNJXU1skZY_GAIc2uC2HIUalM/edit#gid=0), generated from an Elicit.org inquiry.


:::



In spite of the caveats in the fold above, we construct some  measures of aggregate beliefs  using the `aggrecat` package. We will make (and explain) some ad-hoc choices here. We present these:

1. For each paper
2. For categories of papers and cross-paper categories of evaluations
3. For the overall set of papers and evaluations

We can also hold onto these aggregated metrics for later use in modeling.


- Simple averaging

- Bayesian approaches

- Best-performing approaches from elsewhere

- Assumptions over unit-level random terms

### Explicit modeling of 'research quality' (for use in prizes, etc.) {-}

- Use the above aggregation as the outcome of interest, or weight towards categories of greater interest?

- Model with controls -- look for greatest positive residual?


## Inter-rater reliability

## Decomposing variation, dimension reduction, simple linear models


## Later possiblities

- Relation to evaluation text content (NLP?)

- Relation/prediction of later outcomes (traditional publication, citations, replication)



## Scoping our future coverage

We have funding to evaluate roughly 50-70 papers/projects per year, given our proposed incentives.

Consider:

- How many relevant NBER papers come out per year?

- How much relevant work in other prestige archives?

- What quotas do we want (by cause, etc.) and how feasible are these?

