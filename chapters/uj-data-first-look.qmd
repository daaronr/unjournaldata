---
title: "A first look at Unjournal's data"
author: "David Hugh-Jones"
format: html
execute:
  echo: false
editor_options:
  chunk_output_type: console
---

[The Unjournal](https://www.unjournal.org) is an organization aiming to change how
scientific research is evaluated. We carry out journal-independent evaluation
of research papers.

We capture data from our evaluations, including quantitative measures of paper
quality on different dimensions. One of our goals is to use this data to
learn about the evaluation process. Right now, we have only a few dozen evaluations
in the data, so this note just describes some things we *can* do in future, and
shows the code as a proof of concept.

## About the data

Papers^[Actually we don't just evaluate academic papers, but I'll use "papers"
for short.] can be suggested for evaluation either by Unjournal insiders, or by
outsiders. The Unjournal then selects some papers for evaluation. I won't focus on
the details of this process here. Just note that we have more suggested papers
than actual evaluations.

Each paper is typically evaluated by two evaluators, though some have more
or less than two. Getting two or more of every measure is useful, because it
will let us check evaluations against each other.

We ask evaluators two kinds of quantitative questions. First, there are different measures of
*paper quality*. Here they are, along with some snippets from our [guidelines for evaluators](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics):

* *Overall assessment*: "Judge the quality of the research heuristically. Consider all
  aspects of quality, credibility, importance to knowledge production, and
  importance to practice."
* *Advancing our knowledge and practice*: "To what extent does the project
  contribute to the field or to practice, particularly in ways that are relevant
  to global priorities and impactful interventions?..."
* *Methods: Justification, reasonableness, validity, robustness*: "Are the
  methods used well-justified and explained; are they a reasonable approach to
  answering the question(s) in this context? Are the underlying assumptions
  reasonable? Are the results and methods likely to be robust to reasonable
  changes in the underlying assumptions?..."
* *Logic and communication*: "Are the goals and questions of the paper clearly
  expressed? Are concepts clearly defined and referenced? Is the reasoning
  'transparent'? Are assumptions made explicit? Are all logical steps clear and
  correct? Does the writing make the argument easy to follow?"
* *Open, collaborative, replicable science*: "This covers several considerations:
  Replicability, reproducibility, data integrity... Consistency...
  Useful building blocks: Do the authors provide tools, resources, data, and
  outputs that might enable or enhance future work and meta-analysis?"
* *Real-world relevance*: "Are the assumptions and setup realistic and relevant
  to the real world?"
* *Relevance to global priorities*: "Could the paper's topic and approach
  potentially help inform [global priorities, cause prioritization, and
  high-impact interventions](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/the-field-and-ea-gp-research)?

Each of these questions is meant to be a percentile scale, 0-100%, where
the percentage captures the paper's place in the distribution of the reference
group ("all serious research in the same area that you have encountered in the
last three years").^[This 'reference group percentile' interpretation was introduced around the end of 2023; before this evaluators were given the description of the ratings interval seen [here](https://app.gitbook.com/o/-MfFk4CTSGwVOPkwnRgx/s/b1RpEkRWWqZAV4SlrFCt/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators/why-these-guidelines#previous-descriptions-of-ratings-intervals).]
So, for example, a score of 70% would mean the paper is
better than 70% of papers in the reference group. But note, the papers we
evaluate are not randomly sampled from their reference group, so we should not
necessarily expect them to be uniformly distributed on 0-100%.

As well as asking for each question (the midpoint or median of the evaluator's
belief distribution), we also ask for lower and upper bounds of a 90% credible
interval.

Next, we ask two practical questions about publication:

* "What journal ranking tier *should* this work be published in?"

* "What journal ranking tier *will* this work be published in?"

Tiers are measured from 0 ("won't publish/little to no value") up to 5 ("top
journal"). Again, we ask for both an estimate and a 90% credible interval.
We allow non-integer scores between 0 and 5.

The last question is especially interesting, because unlike all the others,
it has an observable ground truth. Eventually, papers do or do not get
published in specific journals, and there is often a consensus
about which journals count as e.g. "top".


## Questions to ask

Here are some things we might hope to learn from our data.

1. Do evaluators understand the questions? Do they "grok" how our percentile
   questions, upper bounds, and lower bounds work?

1. Do evaluators *take the questions seriously*? Or do some of them treat them
   as a nuisance compared to the "real", written review?
   
   Both these questions can be partly addressed by running sanity checks. For
   example, do people "straightline" questions, giving the same answer for
   every question? Do they produce excessively narrow or wide
   confidence intervals?
   
1. Are our quantitative measures *accurate*? Do they capture something "real"
  about the paper? Obviously, we don't have access to "ground truth" -- except
  in one case.

1. Are the different measures *related*? Is there a single underlying dimension
   beneath the different numbers? Or more than one dimension?

1. How do quantitative measures relate to the written, qualitative evaluation?
   Does a more positive written evaluation also score higher on the numbers? 
   Can you predict the numbers from the evaluation?

1. Do evaluators understand the questions *in the same way*? Are different
   evaluators of the same paper answering the "same questions" in their head?
   What about evaluators of different papers in different fields?

1. Do papers score differently in *different fields*? This could be because
   evaluators hold papers to different standards in those fields -- or because
   some fields do genuinely better on some dimensions. We could ask the same
   question about different methodologies: for example, do randomized controlled
   trials score differently than other approaches?


```{r}
#| label: setup

library(conflicted)
conflicts_prefer(dplyr::select, dplyr::filter, .quiet = TRUE)
suppressPackageStartupMessages({
  library(here)
  library(readr)
  library(dplyr)
  library(huxtable)
  library(irr)
  library(purrr)
  library(glue)
})
evals <- readr::read_csv(here("data/evals.csv"), show_col_types = FALSE)

qual_dimensions <- c("overall", "adv_knowledge", "methods", "logic_comms",
                     "real_world", "gp_relevance", "open_sci")
journal_dimensions <- c("journal_predict", "merits_journal")
all_dimensions <- c(qual_dimensions, journal_dimensions)

evals_wide <- evals %>%
  mutate(.by = crucial_rsx_id,
    eval_num = seq(1, n())
  ) %>%
  select(crucial_rsx_id, eval_num, all_of(all_dimensions)) %>%
  tidyr::pivot_wider(
    names_from = eval_num,
    values_from = ends_with(all_dimensions)
  )

n_papers <- nrow(evals_wide)
n_evals <- nrow(evals)

```

## Sanity checks

```{r}
#| label: sanity-checks

row_min_qual <- apply(evals[qual_dimensions], 1, min, na.rm = TRUE)
row_max_qual <- apply(evals[qual_dimensions], 1, max, na.rm = TRUE)

qual_lbs <- paste0("lb_", qual_dimensions)
qual_ubs <- paste0("ub_", qual_dimensions)

# straightliners give the same value for every question,
# so minimum and maximum values are the same
suppressWarnings({
  row_min_qual_lbs <- apply(evals[qual_lbs], 1, min, na.rm = TRUE)
  row_max_qual_lbs <- apply(evals[qual_lbs], 1, max, na.rm = TRUE)
  row_min_qual_ubs <- apply(evals[qual_ubs], 1, min, na.rm = TRUE)
  row_max_qual_ubs <- apply(evals[qual_ubs], 1, max, na.rm = TRUE)
})

straightlines_qual <- row_min_qual == row_max_qual
straightlines_lbs <- row_min_qual_lbs == row_max_qual_lbs
straightlines_ubs <- row_min_qual_ubs == row_max_qual_ubs

n_straightliners <- sum(straightlines_qual)
n_straightliners_lbs <- sum(straightlines_lbs)
n_straightliners_ubs <- sum(straightlines_ubs)

degenerate_cis <- evals[qual_lbs] == evals[qual_ubs]
n_degenerate <- sum(degenerate_cis, na.rm = TRUE)

uninformative_cis <- evals[qual_ubs] - evals[qual_lbs] >= 100
n_uninformative <- sum(uninformative_cis, na.rm = TRUE)

n_valid_cis <- length(na.omit(degenerate_cis))
```

Straightliners are evaluators who give the same score for every question.
For the midpoints, we have `{r} n_straightliners` straightliners out of 
`{r} n_evals` evaluations. We also check if people straightline lower bounds
of the credible intervals (`{r} n_straightliners_lbs` straightliners) and upper
bounds (`{r} n_straightliners_ubs` straightliners).

Evaluators might also give "degenerate" credible intervals, with the lower bound 
equal to the upper bound; or uninformatively wide intervals, with the lower
and upper bounds equal to 0% and 100%. Out of `{r} n_valid_cis` evaluation
questions for which we had valid confidence intervals, `{r} n_degenerate`
were degenerate and `{r} n_uninformative` were uninformative. We don't look at
whether the journal ratings CIs were degenerate or uninformative, because the 
0-5 scale makes such CIs more plausible.

## Accuracy

We have no ground truth of whether a given paper scores high or low on our
7 dimensions. But because we usually have multiple evaluations per paper, we
can take an indirect route. If two evaluators' scores are correlated with
reality, they will also correlate with each other. The converse does not
necessarily hold: evaluators' scores might be correlated because they both
have similar prejudices or both misinterpret the paper in the same way. All
the same, high "inter-rater reliability" (IRR) should increase our confidence
that our scores are measuring something.

IRR is complex. The basic form of most IRR statistics is

$$
\frac{p_a - p_e}{1 - p_e}
$$

where $p_a$ is the proportion of the time that two raters agree, and $p_e$ is the
amount of agreement you'd expect by chance if both raters are choosing
independently.

Why not use $p_a$ directly? Well, for example, suppose our raters pick an expected
journal tier at random, from 0 to 5 inclusive. Clearly there's no reliability:
the data is just random noise. But one time in six, both raters will agree,
simply by chance. So we need to adjust for the expected amount of agreement.
To do this most measures use the marginal distributions of the ratings: in
our example, a 1 in 6 chance of each number from 0 to 5, giving $p_e = 1/6$.
Krippendorff's alpha is a widely accepted statistic that corrects for $p_e$
and also defines "agreement" appropriately for different levels of measurement.

```{r}
#| label: tbl-kripp-alpha


kr_alphas <- list()

for (d in all_dimensions) {
  eval_matrix <- evals_wide %>% select(starts_with(d))
  eval_matrix <- t(eval_matrix)
  kr_alphas[[d]] <- irr::kripp.alpha(eval_matrix, method = "ratio")
  #DR: Don't we want method = "interval" here? The percentile scale has no sense of 'twice as good' nor a true zero, does it?
}

kr_alpha_values <- purrr::list_transpose(kr_alphas)$value

huxtable(
  Dimension = all_dimensions,
  `Krippendorff's Alpha` = kr_alpha_values
) %>%
  set_caption(
    glue("Krippendorf's alpha statistics for our quantitative measures. N = {n_papers} papers, {n_evals} evaluations.")
  )

```


<!--
DR notes and questions: We should probably reference the precise formula for the alpha measure we are using here, or at least make it clear what method this is.

While our notes in `inter-rater-reliability.R` suggested that ICC might be a better for 'ordinal/interval data' I asked chatgpt see [here](https://chatgpt.com/share/ad6a9bac-102d-4f6e-95c2-11bf39eac960) and it seemed to think this is appropriate, and noted it uses a (normalized) squared difference metric. It's comments actually make me think it's better than ICC for this purpose, nice. But we should probably clarify this especially because your example above seemed tp emphasize a small number of categories. Also, see my note above; I'm not sure why we you chose the ratio method rather than the interval method for the percentile data. (Similarly for the journal tier data I think).

-->

<!--
## DR: To do -- Some other things we might want here

- Do confidence intervals or hypothesis test for the K-alphas make sense here? To account for sampling variation of course.

DHJ they make sense but probably provide unnecessary info. 

- Characterize what values of K-alpha are seen as 'reasonable agreement' ... compare it to other contexts. 

- Comparisons by field etc. (I know that's coming, even maybe a select it dashboard).

DHJ will do but right now it would really be too little data to be informative.

- Perhaps a specific take on the numbers above, noting that this is hard-coded as of a certain date and may not reflect updated data.

DHJ: I suggest that we wait till we've got better data before publicizing this.
For now, just use it internally.


-->

Because we have each rater's 90% credible interval, we can also ask a slightly
different question: do raters tend to agree that each other's estimates are
"reasonable"? That is, is rater 1's midpoint estimate within rater 2's
central credible interval, and vice versa?


```{r}
#| label: tbl-credible-intervals


evals_long <- evals %>%
  select(id, crucial_rsx_id, ends_with(all_dimensions)) %>%
  tidyr::pivot_longer(
    cols = ends_with(all_dimensions),
    names_to = c("measure", "dimension"),
    names_pattern = "(lb_|ub_|conf_|)(.*)"
  )  %>%
  mutate(
    measure = if_else(measure == "", "midpoint", measure),
    measure = gsub("_$", "", measure)
  ) %>%
  tidyr::pivot_wider(
    names_from = measure
  )

# This gives a dataset with each paper-dimension in a separate row, but all
# evaluators, and midpoints/lb/ub/confidence levels, in a single row.
evals_long_dim <- evals_long %>%
  mutate(.by = c(crucial_rsx_id, dimension),
    eval_num = consecutive_id(id),
    id = NULL
  ) %>%
  tidyr::pivot_wider(
    names_from = c(eval_num),
    values_from = c(midpoint, lb, ub, conf)
  )

evals_long_dim %>%
  mutate(
    mp1ci2 = between(midpoint_1, lb_2, ub_2),
    mp1ci3 = between(midpoint_1, lb_3, ub_3),
    mp2ci1 = between(midpoint_2, lb_1, ub_1),
    mp2ci3 = between(midpoint_2, lb_3, ub_3),
    mp3ci1 = between(midpoint_3, lb_1, ub_1),
    mp3ci2 = between(midpoint_3, lb_2, ub_2)
  ) %>%
  summarize(.by = dimension,
    `Proportion within C.I.` = mean(c_across(matches("mp.ci")), na.rm = TRUE)
  ) %>%
  rename(
    Dimension = dimension
  ) %>%
  as_huxtable() %>%
  set_number_format(-1, 2, fmt_percent()) %>%
  set_caption(glue("Proportions of midpoints within other evaluators' 90% credible intervals. N = {n_papers} papers, {n_evals} evaluations."))
```

The table above already looks a bit worrying: typically no more than half of
our evaluators' midpoints fall within their co-evaluator's 90% credible interval.
This suggests that our evaluators may be overconfident.

<!-- DR: But this might be driven by some of the evaluators giving degenerate CIs or leaving some categories blank? DHJ: blanks wouldn't be counted. I checked
for degenerate CIs, none visible-->

### More accuracy: predicted and actual publication venues

For the question "What journal ranking tier *will* this work be published in?"
evaluators give a tier from 0-5 with 5 being top, and 0 being no publication.
This gives us a source of ground truth when the evaluated paper gets published.

What counts as a tier 1-5 journal? Several different organizations maintain
journal tier lists, and Professor Anne-Wil Harzing has 
[collated 11 of them](https://harzing.com/resources/journal-quality-list).
Different organizations cover different journals. To get round this we 
impute missing data, and take the first principal component. Luckily, it turns
out that this correlates highly with most of the individual rankings
(@tbl-journals-princomp).^[The calibration code is available [from our Github repository](https://github.com/unjournal/unjournaldata/blob/main/code/calibrate-journal-stats.R).]

```{r}
#| label: tbl-journals-princomp
#| tbl-cap: Correlations of different journal rankings and their principal
#|   component

jql <- readr::read_csv(here("data/jql-enriched.csv"), 
                       show_col_types = FALSE)

jql %>% 
  select(c(princomp1, Cnrs_n, ABDC_n, ABS_n, Den_n, Fnege_n, JourQual_n)) %>% 
  rename_with(\(x) sub("_n$", "", x)) %>% 
  as.matrix() %>% 
  cor(use = "pairwise") %>% 
  as_huxtable() %>% 
  huxtable::add_colnames(rowname = "") %>% 
  huxtable::add_rownames() %>% 
  map_background_color(-1, -1, by_colorspace("red", "yellow")) %>% 
  set_all_borders()
```

The exception is the "Den" ranking (from the Danish Ministry). We
checked whether "Den" added any power beyond the first principal
component to predict citations or h-index data, and it didn't.

To calibrate the principal component, we ran a series of ordinal logistic
models predicting the different tier lists. We used just the "JourQual",
"Cnrs", "ABS" and "Fnege" rankings, since these all had 5 tiers. Category
cutoffs looked similar across all 4 rankings, so we simply took the averages to create our own meta-tier list. @fig-journal-rankings shows the resulting histogram of journals.

```{r}
#| label: fig-journal-rankings
#| fig-cap: Histogram of journal rankings by the Unjournal Tier List

palette("Dark 2")
barplot(table(jql$unjournal_tier), col = 1:5, border = NA,
        xlab = "Unjournal tier", ylab = "Number of journals",
        ylim = c(0, 400))
```

## Relatedness and dimensionality

We have 7 questions measuring paper quality, and 2 questions about journal tier.
We can perform a simple factor analysis of these 9 questions. We don't
yet have enough data to decide the appropriate number of factors, so we just
try three. @tbl-loadings shows loadings. Factor 1 captures 4 of the
paper quality questions, especially real world relevance. Factor 2 correlates with
the journal measures. Factor 3
captures the "open science" quality measure, and to some degree the "methods"
and "logic and communication" measures too.

```{r}
#| label: tbl-loadings
#| tbl-cap: Factor loadings of 3 factors


fa <- factanal(na.omit(evals[all_dimensions]), factors = 3)

ldg <- loadings(fa)
ldg <- unclass(ldg)

as_hux(ldg, add_colnames = TRUE, add_rownames = "Question") %>% 
  set_text_color("white") %>% 
  set_background_color("black") %>% 
  style_header_rows(bold = TRUE) %>% 
  set_all_borders(brdr(color = "white")) %>% 
  map_text_color(-1, -1, by_colorspace("red", "yellow")) 
```


