# Evaluation data: input/features

```{r}
#| warning: false
#| label: load-packages
#| code-summary: "load packages"

library(tidyverse) 

# markdown et al. ----
library(knitr)
library(bookdown)
library(quarto)
library(formattable)

# others ----
library(here)
# library(aggreCAT)
library(DescTools)
select <- dplyr::select 

source(here("code", "DistAggModified.R"))

options(knitr.duplicate.label = "allow")

```


::: {.callout-note collapse="true"}
## Note on data input (10-Aug-23)

Below, the evaluation data is input from an Airtable, which itself was largely hand-input from evaluators' reports. As PubPub builds (target: end of Sept. 2023), this will allow us to  include the ratings and predictions as structured data objects. We then plan to access and input this data *directly* from the PubPub (API?) into the present analysis. This will improve automation and limit the potential for data entry errors.

::: 

```{r read-data}

all_papers_p <- readRDS(here("data/all_papers_p.Rdata"))
evals_pub <- readRDS(here("data/evals.Rdata"))

```


```{r evals_pub to longer format}
evals_pub_long <- evals_pub %>% 
  pivot_longer(cols = -c(id, crucial_rsx, crucial_rsx_id, 
                         paper_abbrev, eval_name, 
                         cat_1,cat_2, cat_3, source_main, author_agreement),
               names_pattern = "(lb_|ub_|conf_)?(.+)",
               names_to = c("value_type", "rating_type")) %>% # one line per rating type
  mutate(value_type = if_else(value_type == "", "est_", value_type)) %>% #add main rating id
  pivot_wider(names_from = value_type, 
              values_from = value)
```


<!-- need airtable API stuff -->

### Reconcile uncertainty ratings and CIs {-}

Where people gave only confidence level 'dots', we impute CIs (confidence/credible intervals). We follow the correspondence described [here](https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#1-5-dots-explanation-and-relation-to-cis). (Otherwise, where they gave actual CIs, we use these.)^[Note this is only a first-pass; a more sophisticated approach may be warranted in future.]

::: {.callout-note collapse="true"}
## Dots to interval choices

> 5 = Extremely confident, i.e., 90% confidence interval spans +/- 4 points or less)

For 0-100 ratings, code the LB as $max(R - 4,0)$ and the UB as $min(R + 4,100)$, where R is the stated (middle) rating. This 'scales' the CI, as interpreted, to be proportional to the rating, with a  maximum 'interval' of about 8, with the rating is about 96.

> 4 = Very*confident: 90% confidence interval +/- 8 points or less

For 0-100 ratings, code the LB as $max(R - 8,0)$ and the UB as $min(R + 8,100)$, where R is the stated (middle) rating. 

> 3 = Somewhat** confident: 90% confidence interval +/- 15 points or less&#x20;

> 2 = Not very** confident: 90% confidence interval, +/- 25 points or less

Comparable scaling for the 2-3 ratings as for the 4 and 5 rating.

> 1 = Not** confident: (90% confidence interval +/- more than 25 points)
    
Code LB as $max(R - 37.5,0)$ and the UB as $min(R + 37.5,100)$. 
    
This is just a first-pass. There might be a more information-theoretic way of doing this. On the other hand, we might be switching the evaluations to use a different tool soon, perhaps getting rid of the 1-5 confidence ratings altogether.

::: 


```{r reconcile_bounds}
#| label: reconcile_bounds
#| code-summary: "reconcile explicit bounds and stated confidence level"

# Define the baseline widths for each confidence rating
# JB: it would be good to have some more backing of whether this
# is a valid way to translate these confidence levels into CIs

# baseline_widths <- c(4, 8, 15, 25, 37.5)

# Lists of categories
rating_cats <- c("overall", "adv_knowledge", "methods", "logic_comms", "real_world", "gp_relevance", "open_sci")

#... 'predictions' are currently 1-5 (0-5?)
pred_cats <- c("journal_predict", "merits_journal")

#DR: Note that I use these objects in future chapters, but they are not connected to the data frame. Either one saves and reinputs the whole environment (messy, replicability issue), or you save this somewhere and re-input it or connect it to the data frame that gets saved (not sure how to do it), or you hard-code reinput it in the next chapter. 

#I do the latter for now, but I'm not happy about it, because the idea is 'input definitions in a single place to use later'

#JB: I don't really understand why not just hard code it in the next chapter. These are very short strings. Do you expect them to change often? If so, we can derive them from a dataframe somewhere in the future or save as a separate object. 


# JB: Rewritten functions for adding imputed ub and lb

# calculate the lower and upper bounds, 
# rating: given a rating, 
# conf: a confidence (1-5) score,
# type: a bound type (lower, upper),
# scale: a scale (100 is 0-100, 5 is 1-5)
# This function is not vectorized
calc_bounds <- function(rating, conf, type, scale) {
  
  if(scale == 5){ #for the 'journal tier prediction case'
    baseline_width = case_match(conf, 
                                5 ~ .2, #4*5/100
                                4 ~ .4, #8*5/100
                                3 ~ .75, #15*5/100
                                2 ~ 1.25, #25*5/100
                                1 ~ 1.875, #37.5*5/100
                                .default = NA_real_) 

    upper = min(rating + baseline_width, 5)
    lower = max(rating - baseline_width, 1)
  }#/if(scale == 5)
  
  if(scale == 100){ #for the 'ratings case'
    
    baseline_width = case_match(conf, 
                                5 ~ 4, 
                                4 ~ 8, 
                                3 ~ 15, 
                                2 ~ 25, 
                                1 ~ 37.5,
                                .default = NA_real_)
    
    upper = min(rating + baseline_width, 100)
    lower = max(rating - baseline_width, 0)
  } #/if(scale == 100)
  
  if(type == "lower") return(lower)
  if(type == "upper") return(upper)
}


# calculate or find correct lower or upper bound
# based on rating type, and lb, ub, and conf values
impute_bounds <- function(var_name, est, lb, ub, conf, bound_type) {
  
  # get scale of each variable
  scale = if_else(var_name %in% c("journal_predict", "merits_journal"), # if variable is a prediction
                  5, 100) #scale is 5, else scale is 100
  # if calculating lower bound variable
  if(bound_type == "lower") { #we are calculating a lower bound imputation
    # 
    calculated_bound = map_dbl(.x = est, .f = calc_bounds, conf = conf, type = bound_type, scale = scale)
    
    imp_bound = if_else(is.na(lb), calculated_bound, lb)
  }
  
  # if calculating upper bound variable
  if(bound_type == "upper") { #we are calculating an upper bound imputation
    # 
    calculated_bound = map_dbl(.x = est, .f = calc_bounds, conf = conf, type = bound_type, scale = scale)
    imp_bound = if_else(is.na(ub), calculated_bound, ub)
  }
  
  return(imp_bound)
}

# apply functions to evals_pub_long
# where each row is one type of rating
# so each evaluation is 9 rows long
evals_pub_long <- evals_pub_long %>% 
  rowwise() %>% # apply function to each row
  mutate(lb_imp_ = impute_bounds(var_name = rating_type,
                                   est = est_,
                                   lb = lb_, ub = ub_, conf = conf_,
                                   bound_type = "lower")) %>% 
  mutate(ub_imp_ = impute_bounds(var_name = rating_type,
                                   est = est_,
                                   lb = lb_, ub = ub_, conf = conf_,
                                   bound_type = "upper"))

# Reshape evals_pub_long into evals_pub to add imputed bounds
evals_pub <- evals_pub_long %>% 
  pivot_wider(names_from = rating_type, # take the dataframe back to old format
              values_from = c(est_, ub_, lb_, conf_, lb_imp_, ub_imp_),
              names_sep = "") %>% 
  dplyr::rename_with(.cols = matches("^[ul]b_imp"),
                     .fn = gsub,
                     pattern = "(ub_imp|lb_imp)_(.+)", 
                     replacement = "\\2_\\1") %>% 
  dplyr::rename_with(.cols = starts_with("est_"),
                     .fn = gsub,
                     pattern = "est_(.+)",
                     replacement = "\\1")

# Clean evals_pub_long names (remove _ at end)
evals_pub_long <- evals_pub_long %>% 
  rename_with(.cols = ends_with("_"),
              .fn = str_remove,
              pattern = "_$")

```


We cannot publicly share the 'papers under consideration', but we can share some of the statistics on these papers. Let's generate an ID (or later, salted hash) for each such paper, and keep only the shareable features of interest


```{r ratings_agg, warning=FALSE}
#| code-summary: "Create and add aggregated ratings information to evals_pub_long"

# paper_ratings: even longer dataframe (ie tidy)
# renamed to conform to aggreCAT nomenclature

paper_ratings <- evals_pub_long %>% 
  select(id, eval_name, paper_abbrev, rating_type, est, lb_imp, ub_imp) %>% 
  filter(rating_type %in% rating_cats) %>%
  rename(paper_id = paper_abbrev,
         user_name = eval_name,
         three_point_lower = lb_imp,
         three_point_upper = ub_imp,
         three_point_best = est) %>%
  mutate(round = "round_1") %>% 
  pivot_longer(cols = starts_with("three_point"),
               names_to = "element",
               values_to = "value")

# calculate aggregated upper and lower bounds using modified
# aggreCAT function DistributionWAggMOD
paper_agg_ratings <- paper_ratings %>% 
  group_by(rating_type, user_name, paper_id) %>% 
  filter(sum(is.na(value))==0) %>% #remove ratings (best, ub, lb) that have NA values
  group_by(rating_type) %>% 
  nest() %>% 
  mutate(results = map(.x = data, .f = DistributionWAggMOD, 
                       round_2_filter = FALSE, percent_toggle = T)) %>% 
  unnest(results) %>% 
  select(-data) %>% 
  select(paper_id, rating_type, everything()) %>%   
  arrange(paper_id) %>% 
  mutate(across(.cols = c("agg_est", "agg_90ci_lb", "agg_90ci_ub"), .fns = ~.x*100)) %>% 
  rename(agg_method = method)

rm(paper_ratings)

evals_pub_long <- evals_pub_long %>% 
  left_join(paper_agg_ratings, by = c("paper_abbrev" = "paper_id", "rating_type"="rating_type"))

```


```{r shiny_data_explorer}
#| code-summary: "create a dataset to be used as the input to the shiny app"

evals_pub_long %>% 
  mutate(rating_type = factor(rating_type, 
                              levels = c(rating_cats, pred_cats),
                              labels = c("Overall assessment",
                                         "Advances our knowledge & practice",  
                                         "Methods: justification, reasonableness, validity, robustness", 
                                         "Logic and communication", 
                                         "Engages with real-world, impact quantification",
                                         "Relevance to global priorities",
                                         "Open, collaborative, replicable science and methods",
                                         "Predicted Journal", "Merits Journal"))) %>% 
  write_rds(file = here("shinyapp/DataExplorer", "shiny_explorer.rds"))
 
```
