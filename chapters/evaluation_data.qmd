# Evaluation data: description, exploration, checks

```{r, warning=FALSE}
#| label: load-packages
#| code-summary: "load packages"


#devtools::install_github("rethinkpriorities/rp-r-package")
library(rethinkpriorities)

#devtools::install_github("rethinkpriorities/r-noodling-package")
library(rnoodling)

library(here)
source(here::here("code", "shared_packages_code.R"))
library(dplyr)
library(pacman)

p_load(santoku, lme4, huxtable, janitor, emmeans, sjPlot, sjmisc, ggeffects, ggrepel, likert, labelled, plotly, stringr, install=FALSE)

p_load(ggthemes, paletteer, ggridges, install=FALSE)

select <- dplyr::select

options(knitr.duplicate.label = "allow")

options(mc.cores = parallel::detectCores())
#rstan_options(auto_write = TRUE)

#library(hunspell)

#(brms)

#devtools::install_github("bergant/airtabler")
p_load(airtabler)

#remotes::install_github("rmcelreath/rethinking")
#library(rethinking)

```



```{r}
#| label: input_at
#| code-summary: "input from airtable"
 

base_id <- "appbPYEw9nURln7Qg"

# Set your Airtable API key
#Sys.setenv(AIRTABLE_API_KEY = "") 
#this should be set in my .Renviron file


# Read data from a specific view
evals <- air_get(base = "appbPYEw9nURln7Qg", "output_eval") 


```

```{r}
#| label: extract
#| code-summary: "just the useful and publish-able data, clean a bit"

colnames(evals) <- snakecase::to_snake_case(colnames(evals))

evals_pub <- evals %>% 
  dplyr::rename(stage_of_process = stage_of_process_todo_from_crucial_research_2) %>% 
  mutate(stage_of_process = unlist(stage_of_process)) %>% 
  dplyr::filter(stage_of_process == "published") %>% 
    select(id, evaluator_name, crucial_research, overall, lb_overall, ub_overall, conf_index_overall, advancing_knowledge_and_practice, lb_advancing_knowledge_and_practice, ub_advancing_knowledge_and_practice, conf_index_advancing_knowledge_and_practice, methods_justification_reasonableness_validity_robustness, lb_methods_justification_reasonableness_validity_robustness, ub_methods_justification_reasonableness_validity_robustness, conf_index_methods_justification_reasonableness_validity_robustness, logic_communication, lb_logic_communication, ub_logic_communication, conf_index_logic_communication, engaging_with_real_world_impact_quantification_practice_realism_and_relevance, lb_engaging_with_real_world_impact_quantification_practice_realism_and_relevance, ub_engaging_with_real_world_impact_quantification_practice_realism_and_relevance, conf_index_engaging_with_real_world_impact_quantification_practice_realism_and_relevance, relevance_to_global_priorities, lb_relevance_to_global_priorities, ub_relevance_to_global_priorities, conf_index_relevance_to_global_priorities, journal_quality_predict, lb_journal_quality_predict, ub_journal_quality_predict, conf_index_journal_quality_predict, open_collaborative_replicable, conf_index_open_collaborative_replicable, lb_open_collaborative_replicable, ub_open_collaborative_replicable, merits_journal, lb_merits_journal, ub_merits_journal, conf_index_merits_journal)

evals_pub %<>%
   mutate(across(everything(), unlist)) #unlist list columns


```                   



```{r}
#| label: shorten_names
#| code-summary: "Shorten names "

new_names <- c(
  "eval_name" = "evaluator_name",
  "crucial_rsx" = "crucial_research",
  "conf_overall" = "conf_index_overall",
  "adv_knowledge" = "advancing_knowledge_and_practice",
  "lb_adv_knowledge" = "lb_advancing_knowledge_and_practice",
  "ub_adv_knowledge" = "ub_advancing_knowledge_and_practice",
  "conf_adv_knowledge" = "conf_index_advancing_knowledge_and_practice",
  "methods" = "methods_justification_reasonableness_validity_robustness",
  "lb_methods" = "lb_methods_justification_reasonableness_validity_robustness",
  "ub_methods" = "ub_methods_justification_reasonableness_validity_robustness",
  "conf_methods" = "conf_index_methods_justification_reasonableness_validity_robustness",
  "logic_comms" = "logic_communication",
  "lb_logic_comms" = "lb_logic_communication",
  "ub_logic_comms" = "ub_logic_communication",
  "conf_logic_comms" = "conf_index_logic_communication",
  "real_world" = "engaging_with_real_world_impact_quantification_practice_realism_and_relevance",
  "lb_real_world" = "lb_engaging_with_real_world_impact_quantification_practice_realism_and_relevance",
  "ub_real_world" = "ub_engaging_with_real_world_impact_quantification_practice_realism_and_relevance",
  "conf_real_world" = "conf_index_engaging_with_real_world_impact_quantification_practice_realism_and_relevance",
  "gp_relevance" = "relevance_to_global_priorities",
  "lb_gp_relevance" = "lb_relevance_to_global_priorities",
  "ub_gp_relevance" = "ub_relevance_to_global_priorities",
  "conf_gp_relevance" = "conf_index_relevance_to_global_priorities",
  "journal_predict" = "journal_quality_predict",
  "lb_journal_predict" = "lb_journal_quality_predict",
  "ub_journal_predict" = "ub_journal_quality_predict",
  "conf_journal_predict" = "conf_index_journal_quality_predict",
  "open_sci" = "open_collaborative_replicable",
  "conf_open_sci" = "conf_index_open_collaborative_replicable",
  "lb_open_sci" = "lb_open_collaborative_replicable",
  "ub_open_sci" = "ub_open_collaborative_replicable",
  "conf_merits_journal" = "conf_index_merits_journal"
)


evals_pub <- evals_pub %>%
  rename(!!!new_names)

# make the old names into labels

library(stringr)
 
#  Create a list of labels
labels <- str_replace_all(new_names, "_", " ")
labels <- str_to_title(labels)
 
# Assign labels to the dataframe
# for(i in seq_along(labels)) {
#    col_name <- new_names[names(new_names)[i]]
#    label <- labels[i]
#    attr(evals_pub[[col_name]], "label") <- label
#  }
# 

```

<!-- need airtable API stuff -->

### Reconcile the uncertainty ratings and CIs (first-pass) {-}

Impute CIs from stated confidence level 'dots', correspondence loosely described [here](https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#1-5-dots-explanation-and-relation-to-cis)

::: {.callout-note collapse="true"}
## Dots to interval choices

> 5 = Extremely confident, i.e., 90% confidence interval spans +/- 4 points or less)

For 0-100 ratings, code the LB as $min(R - 4\times \frac{R}{100},0)$ and the UB as $max(R + 4\times \frac{R}{100},0)$, where R is the stated (middle) rating. This 'scales' the CI, as interpreted, to be proportional to the rating, with a  maximum 'interval' of about 8, with the rating is about 96.

> 4 = Very*confident: 90% confidence interval +/- 8 points or less

For 0-100 ratings, code the LB as $min(R - 8\times \frac{R}{100},0)$ and the UB as $max(R + 8\times \frac{R}{100},0)$, where R is the stated (middle) rating. 

> 3 = Somewhat** confident: 90% confidence interval +/- 15 points or less&#x20;

> 2 = Not very** confident: 90% confidence interval, +/- 25 points or less

Comparable scaling for the 2-3 ratings as for the 4 and 5 rating.

> 1 = Not** confident: (90% confidence interval +/- more than 25 points)
    
Code LB as $min(R - 37.5\times \frac{R}{100},0)$ and the UB as $max(R + 37.5\times \frac{R}{100},0)$. 
    
This is just a first-pass. There might be a more information-theoretic way of doing this. On the other hand, we might be switching the evaluations to use a different tool soon, perhaps getting rid of the 1-5 confidence ratings.

::: 


```{r}
#| label: reconcile_bounds
#| code-summary: "reconcile explicit bounds and stated confidence level"

# Define the baseline widths for each confidence rating
baseline_widths <- c(4, 8, 15, 25, 37.5)

# Define a function to calculate the lower and upper bounds, where given only an index
calc_bounds <- function(rating, confidence, lb_explicit, ub_explicit, scale=100) {
  # Check if confidence is NA
  if (is.na(confidence)) {
    return(c(lb_explicit, ub_explicit))  # Return explicit bounds if confidence is NA
  } else {
    baseline_width <- baseline_widths[confidence]
    lb <- pmax(rating - baseline_width * rating / scale, 0)
    ub <- pmin(rating + baseline_width * rating / scale, scale)
    return(c(lb, ub))
  }
}

# Function to calculate bounds for a single category
calc_category_bounds <- function(df, category, scale=100) {
  # Calculate bounds
  bounds <- mapply(calc_bounds, df[[category]], df[[paste0("conf_", category)]], df[[paste0("lb_", category)]], df[[paste0("ub_", category)]])
  
  # Convert to data frame and ensure it has the same number of rows as the input
  bounds_df <- as.data.frame(t(bounds))
  rownames(bounds_df) <- NULL
  
  # Add bounds to original data frame
  df[[paste0(category, "_lb_imp")]] <- bounds_df[, 1]
  df[[paste0(category, "_ub_imp")]] <- bounds_df[, 2]
  
  return(df)
}


# Lists of categories

rating_cats <- c("overall", "adv_knowledge", "methods", "logic_comms", "real_world", "gp_relevance", "open_sci")

#... 'predictions' are currently 1-5 (0-5?)
pred_cats <- c("journal_predict", "merits_journal")

# Apply the function to each category
# DR: I don't love this looping 'edit in place' code approach, but whatever
for (cat in rating_cats) {
  evals_pub <- calc_category_bounds(evals_pub, cat, scale=100)
}

for (cat in pred_cats) {
  evals_pub <- calc_category_bounds(evals_pub, cat, scale=5)
}

```




```{r}
#| label: savedatas
#| code-summary: "save data for others' use"


evals_pub %>% saveRDS(file = here("data", "evals.Rdata"))
evals_pub %>% write_csv(file = here("data", "evals.csv"))

  
```

    
# Basic presentation

- Simple data summaries/codebooks/dashboards and visualization

- Composition of research evaluated
     - By field (economics, psychology, etc.)
     - By subfield of economics 
     - By topic/cause area (Global health, economic development, impact of technology, global catastrophic risks, etc.)
     - By source (submitted, identified with author permission, direct evaluation)
     
- Timing of intake and evaluation 


### The distribution of ratings and predictions {-}

- For each category and prediction (overall and by paper)

- By field and topic area of paper

- By submission/selection route

- By evaluation manager


### Relationship among the ratings (and predictions) {-} 

- Correlation matrix

- ANOVA

- PCI

- With other 'control' factors?

- How do the specific measures predict the aggregate ones (overall rating, merited publication)
    - CF 'our suggested weighting'


## Aggregation of expert opinion (modeling)

(Consult, e.g., repliCATS/Hanea and others work; meta-science and meta-analysis approaches)

`aggrecat` package

- Simple averaging

- Bayesian approaches 

- Assumptions over unit-level random terms 

### Explicit modeling of 'research quality' (for use in prizes, etc.) {-}

- Use the above aggregation as the outcome of interest, or weight towards categories of greater interest?

- Model with controls -- look for greatest positive residual?   


## Inter-rater reliability

## Decomposing variation, dimension reduction, simple linear models


## Later possiblities

- Relation to evaluation text content (NLP?)

- Relation/prediction of later outcomes (traditional publication, citations, replication)


