---
title: "A first look at Unjournal's data"
author: "David Hugh-Jones"
date: "2024-07-25"
execute:
  echo: false
editor_options:
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
categories: [data]
---

[The Unjournal](https://www.unjournal.org) is an organization aiming to
change how scientific research is evaluated. We carry out
journal-independent evaluation of research papers.

We capture data from our evaluations, including quantitative measures of
paper quality on different dimensions. One of our goals is to use this
data to learn about the evaluation process. Right now, we have only a
few dozen evaluations in the data, so this note just describes some
things we *can* do in future, and shows the code as a proof of concept.

## About the data

Papers[^uj-data-first-look-1] can be suggested for evaluation either by
Unjournal insiders, or by outsiders. The Unjournal then selects some
papers for evaluation. I won't focus on the details of this process
here. Just note that we have more suggested papers than actual
evaluations.

[^uj-data-first-look-1]: Actually we don't just evaluate academic
    papers, but I'll use "papers" for short.

Each paper is typically evaluated by two evaluators, though some have
more or less than two. Getting two or more of every measure is useful,
because it will let us check evaluations against each other.

We ask evaluators two kinds of quantitative questions. First, there are
different measures of *paper quality*. Here they are, along with some
snippets from our [guidelines for
evaluators](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#quantitative-metrics):

-   *Overall assessment*: "Judge the quality of the research
    heuristically. Consider all aspects of quality, credibility,
    importance to knowledge production, and importance to practice."
-   *Advancing our knowledge and practice*: "To what extent does the
    project contribute to the field or to practice, particularly in ways
    that are relevant to global priorities and impactful
    interventions?..."
-   *Methods: Justification, reasonableness, validity, robustness*: "Are
    the methods used well-justified and explained; are they a reasonable
    approach to answering the question(s) in this context? Are the
    underlying assumptions reasonable? Are the results and methods
    likely to be robust to reasonable changes in the underlying
    assumptions?..."
-   *Logic and communication*: "Are the goals and questions of the paper
    clearly expressed? Are concepts clearly defined and referenced? Is
    the reasoning 'transparent'? Are assumptions made explicit? Are all
    logical steps clear and correct? Does the writing make the argument
    easy to follow?"
-   *Open, collaborative, replicable science*: "This covers several
    considerations: Replicability, reproducibility, data integrity...
    Consistency... Useful building blocks: Do the authors provide tools,
    resources, data, and outputs that might enable or enhance future
    work and meta-analysis?"
-   *Real-world relevance*: "Are the assumptions and setup realistic and
    relevant to the real world?"
-   *Relevance to global priorities*: "Could the paper's topic and
    approach potentially help inform [global priorities, cause
    prioritization, and high-impact
    interventions](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/the-field-and-ea-gp-research)?

Each of these questions is meant to be a percentile scale, 0-100%, where
the percentage captures the paper's place in the distribution of the
reference group ("all serious research in the same area that you have
encountered in the last three years").[^uj-data-first-look-2] So, for
example, a score of 70% would mean the paper is better than 70% of
papers in the reference group. But note, the papers we evaluate are not
randomly sampled from their reference group, so we should not
necessarily expect them to be uniformly distributed on 0-100%.

[^uj-data-first-look-2]: This 'reference group percentile'
    interpretation was introduced around the end of 2023; before this
    evaluators were given the description of the ratings interval seen
    [here](https://app.gitbook.com/o/-MfFk4CTSGwVOPkwnRgx/s/b1RpEkRWWqZAV4SlrFCt/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators/why-these-guidelines#previous-descriptions-of-ratings-intervals).

As well as asking for each question (the midpoint or median of the
evaluator's belief distribution), we also ask for lower and upper bounds
of a 90% credible interval.

Next, we ask two practical questions about publication:

-   "What journal ranking tier *should* this work be published in?"

-   "What journal ranking tier *will* this work be published in?"

Tiers are measured from 0 ("won't publish/little to no value") up to 5
("top journal"). Again, we ask for both an estimate and a 90% credible
interval. We allow non-integer scores between 0 and 5.

The last question is especially interesting, because unlike all the
others, it has an observable ground truth. Eventually, papers do or do
not get published in specific journals, and there is often a consensus
about which journals count as e.g. "top".


## Questions to ask

Here are some things we might hope to learn from our data.

1.  Do evaluators understand the questions? Do they "grok" how our
    percentile questions, upper bounds, and lower bounds work?

2.  Do evaluators *take the questions seriously*? Or do some of them
    treat them as a nuisance compared to the "real", written review?

    Both these questions can be partly addressed by running sanity
    checks. For example, do people "straightline" questions, giving the
    same answer for every question? Do they produce excessively narrow
    or wide confidence intervals?

3.  Are our quantitative measures *accurate*? Do they capture something
    "real" about the paper? Obviously, we don't have access to "ground
    truth" -- except in one case.

4.  Are the different measures *related*? Is there a single underlying
    dimension beneath the different numbers? Or more than one dimension?

5.  How do quantitative measures relate to the written, qualitative
    evaluation? Does a more positive written evaluation also score
    higher on the numbers? Can you predict the numbers from the
    evaluation?

6.  Do evaluators understand the questions *in the same way*? Are
    different evaluators of the same paper answering the "same
    questions" in their head? What about evaluators of different papers
    in different fields?

7.  Do papers score differently in *different fields*? This could be
    because evaluators hold papers to different standards in those
    fields -- or because some fields do genuinely better on some
    dimensions. We could ask the same question about different
    methodologies: for example, do randomized controlled trials score
    differently than other approaches?

```{r}
#| label: setup

library(conflicted)
conflicts_prefer(dplyr::select, dplyr::filter, .quiet = TRUE)
suppressPackageStartupMessages({
  library(here)
  library(readr)
  library(dplyr)
  library(huxtable)
  library(irr)
  library(purrr)
  library(glue)
})
options(huxtable.long_minus = TRUE)

research <- readr::read_csv(here("data/research.csv"), show_col_types = FALSE) 
ratings <- readr::read_csv(here("data/rsx_evalr_rating.csv"), 
                           show_col_types = FALSE)
paper_authors <- readr::read_csv(here("data/paper_authors.csv"),  
                                 show_col_types = FALSE) 

n_papers <- n_distinct(ratings$research)
n_evals <- n_distinct(ratings$research, ratings$evaluator)

qual_dimensions <- c("overall", "adv_knowledge", "methods", "logic_comms",
                     "real_world", "gp_relevance", "open_sci")
journal_dimensions <- c("journal_predict", "merits_journal")
all_dimensions <- c(qual_dimensions, journal_dimensions)

dupe_ratings <- (ratings %>% 
  count(research, criteria, evaluator) %>% 
  pull(n)) > 1L
if (any(dupe_ratings)) warning("Duplicate rows found in ratings data.")

# This is temporary and should be removed once the duplicate rows are fixed.
# Otherwise it will hide the broken data, when it should be found and fixed.
ratings <- ratings %>% 
  distinct(.keep_all = TRUE,
    research, evaluator, criteria
  )

# Again, this is a temporary hack until our data is solid.
ratings <- ratings %>% 
  tidyr::drop_na(research, evaluator, criteria)

```

## Sanity checks

```{r}
#| label: sanity-checks

# We suppress warnings about "no non-missing arguments to min/max":
suppressWarnings({
  problem_ratings <- ratings %>% 
    filter(criteria %in% all_dimensions) %>% 
    summarize(.by = c(research, evaluator),
      all_same     = min(middle_rating, na.rm = TRUE) == max(middle_rating, 
                                                             na.rm = TRUE),
      all_same_lo  = min(lower_CI, na.rm = TRUE) == max(lower_CI, na.rm = TRUE),
      all_same_hi  = min(upper_CI, na.rm = TRUE) == max(upper_CI, na.rm = TRUE),
      degen_ci     = sum(criteria %in% qual_dimensions & 
                         lower_CI == upper_CI, 
                         na.rm = TRUE),
      uninf_ci     = sum(criteria %in% qual_dimensions & 
                         upper_CI - lower_CI >= 100, 
                         na.rm = TRUE),
      bad_ci       = sum(
                         lower_CI > upper_CI | 
                         lower_CI > middle_rating |
                         upper_CI < middle_rating, 
                         na.rm = TRUE)
    )
})

n_straightliners    <- sum(problem_ratings$all_same)
n_straightliners_lo <- sum(problem_ratings$all_same_lo)
n_straightliners_hi <- sum(problem_ratings$all_same_hi)

n_degenerate    <- sum(problem_ratings$degen_ci)
n_uninformative <- sum(problem_ratings$uninf_ci)
n_misspecified  <- sum(problem_ratings$bad_ci)
```

Straightliners are evaluators who give the same score for every
question. For the midpoints, we have `{r} n_straightliners`
straightliners out of `{r} n_evals` evaluations. We also check if people
straightline lower bounds of the credible intervals
(`{r} n_straightliners_lo` straightliners) and upper bounds
(`{r} n_straightliners_hi` straightliners).

Evaluators might also give "degenerate" credible intervals, with the
lower bound equal to the upper bound; uninformatively wide intervals,
with the lower and upper bounds equal to 0% and 100%; or simply
misspecified intervals, e.g. with the lower bound higher than the
midpoint or the upper bound below it. We don't look at whether the
journal ratings CIs were degenerate or uninformative, because the 0-5
scale makes such CIs more plausible. Out of `{r} nrow(ratings)`
confidence intervals, `{r} n_degenerate` were degenerate,
`{r} n_uninformative` were uninformative and `{r} n_misspecified` were
misspecified.

## Accuracy

We have no ground truth of whether a given paper scores high or low on
our 7 dimensions. But because we usually have multiple evaluations per
paper, we can take an indirect route. If two evaluators' scores are
correlated with reality, they will also correlate with each other. The
converse does not necessarily hold: evaluators' scores might be
correlated because they both have similar prejudices or both
misinterpret the paper in the same way. All the same, high "inter-rater
reliability" (IRR) should increase our confidence that our scores are
measuring something.

IRR is complex. The basic form of most IRR statistics is

$$
\frac{p_a - p_e}{1 - p_e}
$$

where $p_a$ is the proportion of the time that two raters agree, and
$p_e$ is the amount of agreement you'd expect by chance if both raters
are choosing independently.

Why not use $p_a$ directly? Well, for example, suppose our raters pick
an expected journal tier at random, from 0 to 5 inclusive. Clearly
there's no reliability: the data is just random noise. But one time in
six, both raters will agree, simply by chance. So we need to adjust for
the expected amount of agreement. To do this most measures use the
marginal distributions of the ratings: in our example, a 1 in 6 chance
of each number from 0 to 5, giving $p_e = 1/6$. Krippendorff's alpha is
a widely accepted statistic that corrects for $p_e$ and also defines
"agreement" appropriately for different levels of measurement.

::: callout-note
## Choosing a reliability statistic

There are many ways to measure inter-rater reliability. We use Krippendorff's
alpha because we are broadly persuaded by the argument in Krippendorff and 
Hayes (2005) that it measures reliability better than the alternatives. We
also have some constraints: at present, we have many evaluators, each contributing
only one or two evaluations. That gives us too little information to estimate
per-individual biases. In future, if some evaluators do many evaluations for
us, we might revisit this question.

We use the alpha statistic for a ratio scale, because our ratings are meant to 
be quantiles, which have a natural scale and zero. And we only use papers
with exactly two evaluations. There is a single paper with three evaluations;
adding this in would give us many missing values in the "third
evaluation" column, and we'd have to use more advanced techniques to 
deal with these.
:::

```{r}
#| label: tbl-kripp-alpha


kr_alphas <- list()

for (d in all_dimensions) {
  # This creates a tibble where rows are pieces of research 
  # and columns are different evaluators' ratings.
  r_wide <- ratings %>% 
    filter(criteria == d) %>% 
    select(research, evaluator, middle_rating) %>% 
    # The next two filters are separate, to make sure we have only papers
    # with exactly two *non-NA* ratings.
    filter(! is.na(middle_rating)) %>% 
    filter(.by = research,
      n() == 2L 
    ) %>% 
    mutate(.by = research,
      # This is a trick: if `stopifnot()` doesn't throw an error, `dummy`
      # gets the return value of `NULL` and is never created.
      dummy    = stopifnot(n_distinct(evaluator) == 2L), 
      eval_num = paste0("eval_", seq_len(n()))
    ) %>% 
    select(-evaluator) %>% 
    tidyr::pivot_wider(id_cols = research, names_from = eval_num, 
                       values_from = middle_rating) %>% 
    select(-research)
  
  # We convert this into a matrix where *rows* are evaluators and
  # *columns* are pieces of research. 
  r_matrix <- as.matrix(r_wide)
  r_matrix <- t(r_matrix)
  kr_alphas[[d]] <- irr::kripp.alpha(r_matrix, method = "ratio")
}

# `list_transpose()` turns a list that looks like
# `kr_alphas[[1]]$value, kr_alphas[[2]]$value, ...`
# into a list that looks like
# `x$value[1], x$value[2], ...`.
kr_alpha_values <- purrr::list_transpose(kr_alphas)$value

huxtable(
  Dimension = all_dimensions,
  `Krippendorff's Alpha` = kr_alpha_values
) %>%
  set_caption(
    glue("Krippendorf's alpha statistics for our quantitative measures. N = {n_papers} papers, {n_evals} evaluations.")
  )

```


Because we have each rater's 90% credible interval, we can also ask a
slightly different question: do raters tend to agree that each other's
estimates are "reasonable"? That is, is rater 1's midpoint estimate
within rater 2's central credible interval, and vice versa?


```{r}
#| label: tbl-credible-intervals

# This gives a dataset with one row per paper per dimension, and all
# ratings and CIs of all evaluators in a single row.
ratings_dims <- ratings %>% 
  mutate(.by = research,
    # This simply numbers the evaluators 1, 2, ... for each piece of research:
    eval_num  = as.numeric(factor(evaluator)),
    evaluator = NULL
  ) %>% 
  tidyr::pivot_wider(id_cols = c(research, criteria), names_from = eval_num,
                     values_from = c(middle_rating, lower_CI, upper_CI,
                                    confidence_level))

# This assumes we have no more than 3 evaluators per paper.
ratings_coverage <- ratings_dims %>%
  mutate(
    mp1ci2 = between(middle_rating_1, lower_CI_2, upper_CI_2),
    mp1ci3 = between(middle_rating_1, lower_CI_3, upper_CI_3),
    mp2ci1 = between(middle_rating_2, lower_CI_1, upper_CI_1),
    mp2ci3 = between(middle_rating_2, lower_CI_3, upper_CI_3),
    mp3ci1 = between(middle_rating_3, lower_CI_1, upper_CI_1),
    mp3ci2 = between(middle_rating_3, lower_CI_2, upper_CI_2)
  ) %>%
  summarize(.by = criteria,
    `Proportion within C.I.` = mean(c_across(matches("mp.ci")), na.rm = TRUE)
  ) 

ratings_coverage %>%
  rename(
    Dimension = criteria
  ) %>%
  as_huxtable() %>%
  set_number_format(-1, 2, fmt_percent()) %>%
  set_caption(glue("Proportions of midpoints within other evaluators' 90% credible intervals. N = {n_papers} papers, {n_evals} evaluations."))
```

The table above already looks a bit worrying: typically no more than
half of our evaluators' midpoints fall within their co-evaluator's 90%
credible interval. This suggests that our evaluators may be
overconfident.


## Relatedness and dimensionality

We have 7 questions measuring paper quality, and 2 questions about
journal tier. We can perform a simple principal components analysis of these 9
questions. @tbl-loadings shows loadings for the first three components.


```{r}
#| label: tbl-loadings
#| tbl-cap: Loadings of first 3 principal components on ratings

mean_ratings <- ratings %>% 
  summarize(.by = c(research, criteria),
    mean_rating = mean(middle_rating, na.omit = TRUE)          
  )

mean_ratings_wide <- mean_ratings %>% 
  tidyr::pivot_wider(id_cols = research, names_from = criteria, 
                     values_from = mean_rating)

pc <- princomp(na.omit(mean_ratings_wide[all_dimensions]))

ldg <- loadings(pc)
ldg <- unclass(ldg)

as_hux(ldg[, 1:3], add_colnames = TRUE, add_rownames = "Question") %>% 
  style_header_rows(bold = TRUE) %>% 
  set_align(-1, -1, ".") %>% 
  map_text_color(-1, -1, by_colorspace("blue", "grey", "red")) 
```
